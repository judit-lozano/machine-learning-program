{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">#04. Why Neural Networks Deeply Learn a Mathematical Formula?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Book + Private Lessons [Here ‚Üó](https://sotastica.com/reservar)\n",
    "- Subscribe to my [Blog ‚Üó](https://blog.pythonassembly.com/)\n",
    "- Let's keep in touch on [LinkedIn ‚Üó](www.linkedin.com/in/jsulopz) üòÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning, what does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The Machine Learns...\n",
    ">\n",
    "> But, **what does it learn?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the Machine Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Practical Example ‚Üí [Tesla Autopilot](https://www.tesla.com/AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Example where It Fails ‚Üí [Tesla Confuses Moon with Semaphore](https://twitter.com/Carnage4Life/status/1418920100086784000?s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Simply execute the following lines of code to load the data.\n",
    "> - This dataset contains **statistics about Car Accidents** (columns)\n",
    "> - In each one of **USA States** (rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/fivethirtyeight/fivethirtyeight-bad-drivers-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MT</th>\n",
       "      <td>21.4</td>\n",
       "      <td>8.346</td>\n",
       "      <td>9.416</td>\n",
       "      <td>17.976</td>\n",
       "      <td>18.190</td>\n",
       "      <td>816.21</td>\n",
       "      <td>85.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>14.5</td>\n",
       "      <td>3.625</td>\n",
       "      <td>4.205</td>\n",
       "      <td>13.775</td>\n",
       "      <td>13.775</td>\n",
       "      <td>710.46</td>\n",
       "      <td>108.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL</th>\n",
       "      <td>12.8</td>\n",
       "      <td>4.608</td>\n",
       "      <td>4.352</td>\n",
       "      <td>12.032</td>\n",
       "      <td>12.288</td>\n",
       "      <td>803.11</td>\n",
       "      <td>139.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ND</th>\n",
       "      <td>23.9</td>\n",
       "      <td>5.497</td>\n",
       "      <td>10.038</td>\n",
       "      <td>23.661</td>\n",
       "      <td>20.554</td>\n",
       "      <td>688.75</td>\n",
       "      <td>109.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL</th>\n",
       "      <td>17.9</td>\n",
       "      <td>3.759</td>\n",
       "      <td>5.191</td>\n",
       "      <td>16.468</td>\n",
       "      <td>16.826</td>\n",
       "      <td>1160.13</td>\n",
       "      <td>144.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                       \n",
       "MT       21.4     8.346    9.416          17.976       18.190       816.21   \n",
       "IN       14.5     3.625    4.205          13.775       13.775       710.46   \n",
       "IL       12.8     4.608    4.352          12.032       12.288       803.11   \n",
       "ND       23.9     5.497   10.038          23.661       20.554       688.75   \n",
       "FL       17.9     3.759    5.191          16.468       16.826      1160.13   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "MT           85.15  \n",
       "IN          108.92  \n",
       "IL          139.15  \n",
       "ND          109.72  \n",
       "FL          144.18  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset(name='car_crashes', index_col='abbrev')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Concepts in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the `Weights`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accidents = speeding \\cdot w_1 + alcohol \\cdot w_2 \\ + ... + \\ ins\\_losses \\cdot w_7\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 11:43:40.718882: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-01-08 11:43:40.718977: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='zeros'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 11:43:43.693709: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-08 11:43:43.693895: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-01-08 11:43:43.723294: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-1.164714  ],\n",
       "        [-0.80074704],\n",
       "        [ 0.21468008]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x=AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 205.0333 - mse: 205.0333\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 43.6000 - mse: 43.6000\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 28.5555 - mse: 28.5555\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 28.3839 - mse: 28.3839\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 28.7821 - mse: 28.7821\n",
      "Epoch 6/500\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 21.4393 - mse: 21.4393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 11:43:46.862258: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 27.5762 - mse: 27.5762\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.6885 - mse: 26.6885\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 26.3911 - mse: 26.3911\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.6578 - mse: 26.6578\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.3040 - mse: 26.3040\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.5792 - mse: 26.5792\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 25.7936 - mse: 25.7936\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 24.5818 - mse: 24.5818\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 25.3552 - mse: 25.3552\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 25.8092 - mse: 25.8092\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 30.3899 - mse: 30.3899\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 29.2364 - mse: 29.2364\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 23.4661 - mse: 23.4661\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.1361 - mse: 26.1361\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 23.8447 - mse: 23.8447\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.4721 - mse: 22.4721\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.3788 - mse: 21.3788\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.7773 - mse: 20.7773\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 20.3464 - mse: 20.3464\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.0470 - mse: 20.0470\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 19.9099 - mse: 19.9099\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.6520 - mse: 19.6520\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.6630 - mse: 18.6630\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 17.9833 - mse: 17.9833\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 29.8869 - mse: 29.8869\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.0758 - mse: 19.0758\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.2382 - mse: 17.2382\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.0471 - mse: 17.0471\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 16.8556 - mse: 16.8556\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 18.9903 - mse: 18.9903\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.3352 - mse: 22.3352\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.7893 - mse: 15.7893\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 16.1003 - mse: 16.1003\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.1079 - mse: 15.1079\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.7833 - mse: 14.7833\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.7131 - mse: 17.7131\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 17.7836 - mse: 17.7836\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.9047 - mse: 13.9047\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.0664 - mse: 14.0664\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.3694 - mse: 14.3694\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.7157 - mse: 13.7157\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.6705 - mse: 13.6705\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 13.7581 - mse: 13.7581\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 17.3308 - mse: 17.3308\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 17.3607 - mse: 17.3607\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 17.3677 - mse: 17.3677\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 12.8589 - mse: 12.8589\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.0375 - mse: 12.0375\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.0220 - mse: 11.0220\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.1263 - mse: 13.1263\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.3840 - mse: 12.3840\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.8506 - mse: 10.8506\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.0658 - mse: 10.0658\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.2437 - mse: 10.2437\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.7768 - mse: 10.7768\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 9.4443 - mse: 9.4443\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.2070 - mse: 13.2070\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.1945 - mse: 10.1945\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 9.9800 - mse: 9.9800\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.0162 - mse: 9.0162\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 9.0310 - mse: 9.0310\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.3004 - mse: 10.3004\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.3241 - mse: 12.3241\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.1653 - mse: 8.1653\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.1048 - mse: 8.1048\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.6197 - mse: 7.6197\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.4587 - mse: 7.4587\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.0854 - mse: 7.0854\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 8.7797 - mse: 8.7797\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.7355 - mse: 9.7355\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.5578 - mse: 6.5578\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.2553 - mse: 8.2553\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.6323 - mse: 7.6323\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.1757 - mse: 6.1757\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.2128 - mse: 8.2128\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.9304 - mse: 8.9304\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.9543 - mse: 6.9543\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.4350 - mse: 8.4350\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.5030 - mse: 5.5030\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.7789 - mse: 5.7789\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.4370 - mse: 8.4370\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.6991 - mse: 11.6991\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.1712 - mse: 6.1712\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.0401 - mse: 5.0401\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 4.8758 - mse: 4.8758\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.5913 - mse: 5.5913\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.6083 - mse: 8.6083\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 5.0395 - mse: 5.0395\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5607 - mse: 8.5607\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.9406 - mse: 6.9406\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 4.4003 - mse: 4.4003\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 4.3318 - mse: 4.3318\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.7885 - mse: 4.7885\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.3516 - mse: 7.3516\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.0006 - mse: 6.0006\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1842 - mse: 4.1842\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 11.5340 - mse: 11.5340\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 7.0818 - mse: 7.0818\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0822 - mse: 4.0822\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7811 - mse: 3.7811\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7192 - mse: 3.7192\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.7664 - mse: 3.7664\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.6678 - mse: 3.6678\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.7428 - mse: 3.7428\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7745 - mse: 3.7745\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 6.7293 - mse: 6.7293\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.8106 - mse: 9.8106\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 5.4831 - mse: 5.4831\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.6522 - mse: 3.6522\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.3623 - mse: 3.3623\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.2945 - mse: 3.2945\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.9793 - mse: 4.9793\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.4323 - mse: 7.4323\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.1913 - mse: 8.1913\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1355 - mse: 3.1355\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.2041 - mse: 3.2041\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.0698 - mse: 6.0698\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.9071 - mse: 6.9071\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 5.1289 - mse: 5.1289\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.6179 - mse: 3.6179\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8189 - mse: 2.8189\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.9059 - mse: 2.9059\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.1215 - mse: 3.1215\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.7522 - mse: 3.7522\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.0808 - mse: 5.0808\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.8616 - mse: 3.8616\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.8423 - mse: 3.8423\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.6632 - mse: 2.6632\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8679 - mse: 2.8679\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.1238 - mse: 4.1238\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.1847 - mse: 7.1847\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 6.8432 - mse: 6.8432\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.7460 - mse: 4.7460\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7334 - mse: 2.7334\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3782 - mse: 2.3782\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4525 - mse: 2.4525\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4825 - mse: 2.4825\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.1017 - mse: 7.1017\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.7269 - mse: 5.7269\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5943 - mse: 2.5943\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.9691 - mse: 2.9691\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.1404 - mse: 4.1404\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.0796 - mse: 5.0796\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0646 - mse: 4.0646\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.7351 - mse: 2.7351\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.7565 - mse: 2.7565\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.0293 - mse: 5.0293\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.6220 - mse: 7.6220\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.3537 - mse: 3.3537\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5616 - mse: 2.5616\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3908 - mse: 2.3908\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1292 - mse: 2.1292\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.2740 - mse: 2.2740\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.0760 - mse: 2.0760\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0274 - mse: 4.0274\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.9056 - mse: 10.9056\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6929 - mse: 2.6929\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.9919 - mse: 1.9919\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.9811 - mse: 1.9811\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.1828 - mse: 3.1828\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.2374 - mse: 6.2374\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.6903 - mse: 3.6903\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5236 - mse: 2.5236\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2117 - mse: 2.2117\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0013 - mse: 3.0013\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 3.9689 - mse: 3.9689\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7370 - mse: 3.7370\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9940 - mse: 2.9940\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.7100 - mse: 2.7100\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 4.8836 - mse: 4.8836\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 6.1388 - mse: 6.1388\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.2798 - mse: 3.2798\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.0210 - mse: 2.0210\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9299 - mse: 1.9299\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.7682 - mse: 3.7682\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.0421 - mse: 4.0421\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.6270 - mse: 5.6270\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.6970 - mse: 7.6970\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.0148 - mse: 5.0148\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1348 - mse: 3.1348\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.2372 - mse: 2.2372\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9702 - mse: 1.9702\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.2759 - mse: 2.2759\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.4240 - mse: 3.4240\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.5002 - mse: 3.5002\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6280 - mse: 2.6280\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.3857 - mse: 4.3857\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.0296 - mse: 6.0296\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.5269 - mse: 4.5269\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.7951 - mse: 1.7951\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1718 - mse: 2.1718\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.9773 - mse: 3.9773\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.5870 - mse: 3.5870\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8081 - mse: 1.8081\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8301 - mse: 1.8301\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.6768 - mse: 1.6768\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.4076 - mse: 4.4076\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.0087 - mse: 8.0087\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.1213 - mse: 3.1213\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5641 - mse: 2.5641\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8118 - mse: 1.8118\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.8251 - mse: 1.8251\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4393 - mse: 2.4393\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.3767 - mse: 4.3767\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.1202 - mse: 5.1202\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.3023 - mse: 5.3023\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5111 - mse: 2.5111\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6587 - mse: 1.6587\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9831 - mse: 1.9831\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0170 - mse: 2.0170\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0057 - mse: 4.0057\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2950 - mse: 4.2950\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9623 - mse: 1.9623\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7165 - mse: 2.7165\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.7416 - mse: 5.7416\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.8484 - mse: 4.8484\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.6316 - mse: 3.6316\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.9626 - mse: 3.9626\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3671 - mse: 2.3671\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7354 - mse: 2.7354\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1808 - mse: 2.1808\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.3074 - mse: 3.3074\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 5.0443 - mse: 5.0443\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.8381 - mse: 3.8381\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8680 - mse: 1.8680\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.6313 - mse: 1.6313\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5870 - mse: 1.5870\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5667 - mse: 1.5667\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.6534 - mse: 3.6534\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.0622 - mse: 9.0622\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.6844 - mse: 3.6844\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.8541 - mse: 1.8541\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.5699 - mse: 1.5699\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9978 - mse: 1.9978\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3561 - mse: 2.3561\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.9576 - mse: 1.9576\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.6038 - mse: 1.6038\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.2221 - mse: 4.2221\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 6.3974 - mse: 6.3974\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4834 - mse: 2.4834\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1293 - mse: 2.1293\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.2352 - mse: 3.2352\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5706 - mse: 2.5706\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.5784 - mse: 1.5784\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5565 - mse: 1.5565\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6593 - mse: 1.6593\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.1967 - mse: 2.1967\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.2864 - mse: 8.2864\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.3097 - mse: 8.3097\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1146 - mse: 3.1146\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.8289 - mse: 1.8289\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.7457 - mse: 1.7457\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.6614 - mse: 1.6614\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.5220 - mse: 1.5220\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.5241 - mse: 1.5241\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.5097 - mse: 1.5097\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.5349 - mse: 1.5349\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.9532 - mse: 3.9532\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 11.7027 - mse: 11.7027\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.0316 - mse: 3.0316\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.6691 - mse: 1.6691\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.5561 - mse: 1.5561\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.6987 - mse: 1.6987\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8359 - mse: 1.8359\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6246 - mse: 2.6246\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.8213 - mse: 6.8213\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.8647 - mse: 5.8647\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6433 - mse: 2.6433\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6735 - mse: 1.6735\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4927 - mse: 1.4927\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9058 - mse: 1.9058\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9176 - mse: 1.9176\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5165 - mse: 1.5165\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1260 - mse: 3.1260\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3797 - mse: 2.3797\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1643 - mse: 2.1643\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7871 - mse: 3.7871\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.5633 - mse: 7.5633\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 5.3597 - mse: 5.3597\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7820 - mse: 1.7820\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4436 - mse: 1.4436\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8987 - mse: 1.8987\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.5647 - mse: 3.5647\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.4276 - mse: 5.4276\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8588 - mse: 2.8588\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4458 - mse: 1.4458\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.5326 - mse: 3.5326\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.5680 - mse: 5.5680\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8819 - mse: 2.8819\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4589 - mse: 1.4589\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4772 - mse: 2.4772\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.8829 - mse: 3.8829\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8979 - mse: 2.8979\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4191 - mse: 2.4191\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.6395 - mse: 1.6395\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6836 - mse: 1.6836\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.7766 - mse: 3.7766\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.6784 - mse: 5.6784\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6432 - mse: 2.6432\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7233 - mse: 2.7233\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6090 - mse: 2.6090\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.6298 - mse: 2.6298\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.7788 - mse: 4.7788\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.0723 - mse: 5.0723\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5521 - mse: 2.5521\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9692 - mse: 1.9692\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.0386 - mse: 2.0386\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1116 - mse: 3.1116\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5754 - mse: 1.5754\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8770 - mse: 1.8770\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.7257 - mse: 4.7257\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.5331 - mse: 4.5331\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6428 - mse: 2.6428\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.1118 - mse: 2.1118\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5758 - mse: 2.5758\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.0285 - mse: 2.0285\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.3841 - mse: 4.3841\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.3871 - mse: 4.3871\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2826 - mse: 4.2826\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.9619 - mse: 3.9619\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7299 - mse: 2.7299\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4997 - mse: 2.4997\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2217 - mse: 4.2217\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.6488 - mse: 3.6488\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9825 - mse: 1.9825\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4375 - mse: 2.4375\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.8271 - mse: 3.8271\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.9834 - mse: 2.9834\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.9770 - mse: 3.9770\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9222 - mse: 2.9222\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9636 - mse: 2.9636\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.8096 - mse: 3.8096\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9663 - mse: 1.9663\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6722 - mse: 1.6722\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0789 - mse: 4.0789\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2272 - mse: 4.2272\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3987 - mse: 1.3987\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.6079 - mse: 1.6079\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3550 - mse: 1.3550\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4311 - mse: 1.4311\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9788 - mse: 2.9788\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.5332 - mse: 5.5332\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.1281 - mse: 6.1281\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.1844 - mse: 2.1844\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9510 - mse: 1.9510\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.8459 - mse: 3.8459\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2384 - mse: 3.2384\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3602 - mse: 2.3602\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.0966 - mse: 3.0966\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.4087 - mse: 1.4087\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3991 - mse: 1.3991\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4671 - mse: 1.4671\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7497 - mse: 2.7497\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.8536 - mse: 5.8536\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.5280 - mse: 3.5280\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9194 - mse: 2.9194\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.4950 - mse: 4.4950\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0220 - mse: 2.0220\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.5495 - mse: 1.5495\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.6663 - mse: 2.6663\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.3013 - mse: 4.3013\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1262 - mse: 2.1262\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3268 - mse: 2.3268\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.0306 - mse: 2.0306\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.2039 - mse: 2.2039\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3387 - mse: 2.3387\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8526 - mse: 2.8526\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.8169 - mse: 2.8169\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7509 - mse: 3.7509\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5304 - mse: 2.5304\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7849 - mse: 1.7849\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.2117 - mse: 4.2117\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5084 - mse: 2.5084\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3194 - mse: 1.3194\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3047 - mse: 1.3047\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5698 - mse: 1.5698\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.2425 - mse: 7.2425\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.7667 - mse: 4.7667\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9905 - mse: 1.9905\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.8598 - mse: 1.8598\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6880 - mse: 1.6880\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3731 - mse: 1.3731\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4531 - mse: 1.4531\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0456 - mse: 4.0456\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.2836 - mse: 5.2836\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4744 - mse: 2.4744\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5002 - mse: 2.5002\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.7426 - mse: 1.7426\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.4167 - mse: 1.4167\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2728 - mse: 1.2728\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6560 - mse: 2.6560\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.0571 - mse: 8.0571\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3552 - mse: 2.3552\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4301 - mse: 1.4301\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.2752 - mse: 1.2752\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.2992 - mse: 1.2992\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.2845 - mse: 1.2845\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.2787 - mse: 1.2787\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.8192 - mse: 1.8192\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 7.8990 - mse: 7.8990\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 8.7098 - mse: 8.7098\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2698 - mse: 2.2698\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.3531 - mse: 1.3531\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.4197 - mse: 1.4197\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2519 - mse: 2.2519\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1352 - mse: 2.1352\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.4373 - mse: 1.4373\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8144 - mse: 1.8144\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1237 - mse: 4.1237\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.4186 - mse: 6.4186\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1690 - mse: 2.1690\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.2615 - mse: 1.2615\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.8239 - mse: 1.8239\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5718 - mse: 1.5718\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2401 - mse: 1.2401\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.3440 - mse: 1.3440\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4217 - mse: 2.4217\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.7211 - mse: 8.7211\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.7246 - mse: 2.7246\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8611 - mse: 1.8611\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9376 - mse: 2.9376\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0308 - mse: 2.0308\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2311 - mse: 1.2311\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5891 - mse: 1.5891\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1106 - mse: 4.1106\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.6543 - mse: 7.6543\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.1728 - mse: 5.1728\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.2090 - mse: 2.2090\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8454 - mse: 1.8454\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5429 - mse: 1.5429\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.3761 - mse: 2.3761\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7399 - mse: 3.7399\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.9375 - mse: 3.9375\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9694 - mse: 2.9694\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8444 - mse: 1.8444\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.7201 - mse: 1.7201\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.2605 - mse: 2.2605\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.1629 - mse: 2.1629\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6641 - mse: 1.6641\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0935 - mse: 2.0935\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6237 - mse: 2.6237\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.9019 - mse: 3.9019\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1913 - mse: 4.1913\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.4678 - mse: 4.4678\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2628 - mse: 3.2628\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.0647 - mse: 2.0647\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5112 - mse: 2.5112\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.2845 - mse: 2.2845\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7919 - mse: 1.7919\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.6688 - mse: 3.6688\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.8725 - mse: 3.8725\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.1174 - mse: 3.1174\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.6184 - mse: 3.6184\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.7764 - mse: 3.7764\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2326 - mse: 2.2326\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2159 - mse: 3.2159\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.6470 - mse: 3.6470\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2707 - mse: 3.2707\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2965 - mse: 2.2965\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7651 - mse: 2.7651\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9166 - mse: 2.9166\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2492 - mse: 1.2492\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3407 - mse: 1.3407\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5819 - mse: 1.5819\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.6635 - mse: 3.6635\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.6095 - mse: 3.6095\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.1996 - mse: 5.1996\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1193 - mse: 4.1193\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8169 - mse: 2.8169\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3421 - mse: 1.3421\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2140 - mse: 1.2140\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4045 - mse: 1.4045\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9163 - mse: 2.9163\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.4995 - mse: 3.4995\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0882 - mse: 3.0882\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8899 - mse: 2.8899\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8130 - mse: 1.8130\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6269 - mse: 2.6269\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0711 - mse: 4.0711\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.9893 - mse: 3.9893\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4275 - mse: 1.4275\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3543 - mse: 1.3543\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5317 - mse: 1.5317\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9418 - mse: 1.9418\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.5571 - mse: 3.5571\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1894 - mse: 4.1894\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.6425 - mse: 3.6425\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4680 - mse: 2.4680\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.1771 - mse: 1.1771\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2926 - mse: 1.2926\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2674 - mse: 1.2674\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.6469 - mse: 1.6469\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0220 - mse: 4.0220\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.4327 - mse: 5.4327\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2902 - mse: 3.2902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16c02a340>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-8.8649951e-02, -8.8617638e-02,  8.8457465e-02],\n",
       "        [-1.9501776e-01, -1.9500150e-01,  1.9496396e-01],\n",
       "        [-1.4038005e-01, -1.4035602e-01,  1.4025827e-01],\n",
       "        [-2.1238077e-01, -2.1235867e-01,  2.1226947e-01],\n",
       "        [-8.2143059e-05,  6.4426975e-05, -1.1733393e-03],\n",
       "        [-8.2339048e-03, -8.1040664e-03,  7.1365214e-03]], dtype=float32),\n",
       " array([-0.08605691, -0.0859602 ,  0.08526421], dtype=float32),\n",
       " array([[-1.1976191 ],\n",
       "        [-0.8334849 ],\n",
       "        [ 0.24593592]], dtype=float32),\n",
       " array([0.08628452], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa1666b64c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[17.951685]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.951683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.089252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>17.327856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>21.157095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.099650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit\n",
       "abbrev                             \n",
       "AL       18.8             17.951683\n",
       "AK       18.1             17.089252\n",
       "AZ       18.6             17.327856\n",
       "AR       22.4             21.157095\n",
       "CA       12.0             12.099650"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel = df[['total']].copy()\n",
    "dfsel['pred_zeros_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.078846136666243"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_zeros_after_fit)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accidents = speeding \\cdot w_1 + alcohol \\cdot w_2 \\ + ... + \\ ins\\_losses \\cdot w_7\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 7)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='ones'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa167644dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1665.7175]], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[ 1.0566713],\n",
       "        [-0.115363 ],\n",
       "        [ 0.7659117]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1665.7175]], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 956us/step - loss: 3294665.7500 - mse: 3294665.7500\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 821us/step - loss: 3199035.0000 - mse: 3199035.0000\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 852us/step - loss: 3133813.2500 - mse: 3133813.2500\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 906us/step - loss: 3079202.0000 - mse: 3079202.0000\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 777us/step - loss: 3031185.5000 - mse: 3031185.5000\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 832us/step - loss: 2987089.0000 - mse: 2987089.0000\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 847us/step - loss: 2945393.5000 - mse: 2945393.5000\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 773us/step - loss: 2905242.2500 - mse: 2905242.2500\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 826us/step - loss: 2866507.2500 - mse: 2866507.2500\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 790us/step - loss: 2829034.0000 - mse: 2829034.0000\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2791791.7500 - mse: 2791791.7500\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 941us/step - loss: 2756019.2500 - mse: 2756019.2500\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 858us/step - loss: 2720785.0000 - mse: 2720785.0000\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 783us/step - loss: 2685744.7500 - mse: 2685744.7500\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 764us/step - loss: 2651863.7500 - mse: 2651863.7500\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 813us/step - loss: 2618236.5000 - mse: 2618236.5000\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 957us/step - loss: 2585047.5000 - mse: 2585047.5000\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 781us/step - loss: 2552592.7500 - mse: 2552592.7500\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 770us/step - loss: 2520337.5000 - mse: 2520337.5000\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 820us/step - loss: 2487900.0000 - mse: 2487900.0000\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 758us/step - loss: 2455620.5000 - mse: 2455620.5000\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 760us/step - loss: 2423673.5000 - mse: 2423673.5000\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 879us/step - loss: 2392138.7500 - mse: 2392138.7500\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 774us/step - loss: 2361224.2500 - mse: 2361224.2500\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 747us/step - loss: 2330018.0000 - mse: 2330018.0000\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 744us/step - loss: 2298942.0000 - mse: 2298942.0000\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 758us/step - loss: 2268862.5000 - mse: 2268862.5000\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 745us/step - loss: 2239190.0000 - mse: 2239190.0000\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 823us/step - loss: 2209121.7500 - mse: 2209121.7500\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 749us/step - loss: 2179320.0000 - mse: 2179320.0000\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 720us/step - loss: 2150027.7500 - mse: 2150027.7500\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 825us/step - loss: 2121281.5000 - mse: 2121281.5000\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 842us/step - loss: 2092616.5000 - mse: 2092616.5000\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 2063773.1250 - mse: 2063773.1250\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 764us/step - loss: 2035671.2500 - mse: 2035671.2500\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2007498.6250 - mse: 2007498.6250\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1979035.0000 - mse: 1979035.0000\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 996us/step - loss: 1950623.2500 - mse: 1950623.2500\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 890us/step - loss: 1922621.0000 - mse: 1922621.0000\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 823us/step - loss: 1894704.5000 - mse: 1894704.5000\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 903us/step - loss: 1867519.3750 - mse: 1867519.3750\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 733us/step - loss: 1840477.7500 - mse: 1840477.7500\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 799us/step - loss: 1813550.2500 - mse: 1813550.2500\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 795us/step - loss: 1787021.5000 - mse: 1787021.5000\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 943us/step - loss: 1760679.2500 - mse: 1760679.2500\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 874us/step - loss: 1734452.8750 - mse: 1734452.8750\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 989us/step - loss: 1709002.0000 - mse: 1709002.0000\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1683950.1250 - mse: 1683950.1250\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1658827.8750 - mse: 1658827.8750\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1633677.7500 - mse: 1633677.7500\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1597419.7500 - mse: 1597419.750 - 0s 955us/step - loss: 1608849.1250 - mse: 1608849.0000\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1584268.8750 - mse: 1584268.8750\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1560001.6250 - mse: 1560001.6250\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1535593.7500 - mse: 1535593.7500\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 988us/step - loss: 1511670.1250 - mse: 1511670.1250\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1487868.5000 - mse: 1487868.5000\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1464265.0000 - mse: 1464265.0000\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1441093.1250 - mse: 1441093.1250\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1417880.7500 - mse: 1417880.7500\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1394996.1250 - mse: 1394996.1250\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1372000.6250 - mse: 1372000.6250\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1348937.7500 - mse: 1348937.7500\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1326352.6250 - mse: 1326352.6250\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1303804.1250 - mse: 1303804.1250\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1281906.5000 - mse: 1281906.5000\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 850us/step - loss: 1260479.1250 - mse: 1260479.1250\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 816us/step - loss: 1238907.3750 - mse: 1238907.3750\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 956us/step - loss: 1217238.6250 - mse: 1217238.6250\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1196008.7500 - mse: 1196008.7500\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 916us/step - loss: 1175004.8750 - mse: 1175004.8750\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1154281.2500 - mse: 1154281.2500\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1134041.1250 - mse: 1134041.1250\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 868us/step - loss: 1113920.7500 - mse: 1113920.7500\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 852us/step - loss: 1093920.1250 - mse: 1093920.1250\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 845us/step - loss: 1074049.6250 - mse: 1074049.6250\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 972us/step - loss: 1054383.0000 - mse: 1054383.0000\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 965us/step - loss: 1034854.1250 - mse: 1034854.1250\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1015703.6875 - mse: 1015703.6875\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 846us/step - loss: 996613.3125 - mse: 996613.3125\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 977498.6875 - mse: 977498.6875\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 958851.4375 - mse: 958851.4375\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 926us/step - loss: 940266.1875 - mse: 940266.1875\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 921872.8125 - mse: 921872.8125\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 903840.8750 - mse: 903840.9375\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 893us/step - loss: 885931.3750 - mse: 885931.3750\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 771us/step - loss: 867951.5000 - mse: 867951.5000\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 810us/step - loss: 850376.0000 - mse: 850376.0000\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 887us/step - loss: 833279.5000 - mse: 833279.5000\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 882us/step - loss: 816318.1250 - mse: 816318.1250\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 799252.2500 - mse: 799252.2500\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 782674.1250 - mse: 782674.1250\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 936us/step - loss: 766280.3125 - mse: 766280.3125\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 801us/step - loss: 749921.1250 - mse: 749921.1250\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 855us/step - loss: 733842.5625 - mse: 733842.5625\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 909us/step - loss: 717879.0000 - mse: 717879.0000\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 767us/step - loss: 702146.3750 - mse: 702146.3750\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 870us/step - loss: 686897.5625 - mse: 686897.5625\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 716us/step - loss: 671897.4375 - mse: 671897.4375\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 656729.5000 - mse: 656729.5000\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 641800.1875 - mse: 641800.1875\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 627188.3750 - mse: 627188.3750\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 889us/step - loss: 612662.0625 - mse: 612662.0625\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 832us/step - loss: 598400.1250 - mse: 598400.1250\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 864us/step - loss: 584293.3125 - mse: 584293.3125\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 828us/step - loss: 570265.4375 - mse: 570265.4375\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 822us/step - loss: 556498.1875 - mse: 556498.1875\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 542731.3750 - mse: 542731.3750\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 528949.8125 - mse: 528949.8125\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 515667.2500 - mse: 515667.2500\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 828us/step - loss: 502792.4688 - mse: 502792.4688\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 983us/step - loss: 489855.7500 - mse: 489855.7500\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 477110.8125 - mse: 477110.8125\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 931us/step - loss: 464502.0312 - mse: 464502.0312\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 866us/step - loss: 452047.8438 - mse: 452047.8438\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 899us/step - loss: 440005.9062 - mse: 440005.9062\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 428142.0312 - mse: 428142.0312\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 948us/step - loss: 416288.6250 - mse: 416288.6250\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 404836.9688 - mse: 404836.9688\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 393672.9375 - mse: 393672.9375\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 382568.4688 - mse: 382568.4688\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 371547.9062 - mse: 371547.9062\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 360778.4375 - mse: 360778.4375\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 350059.4375 - mse: 350059.4375\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 986us/step - loss: 339453.1875 - mse: 339453.1875\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 696us/step - loss: 329031.5938 - mse: 329031.5938\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 828us/step - loss: 318848.3750 - mse: 318848.4062\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 308954.0312 - mse: 308954.0312\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 299371.1875 - mse: 299371.1875\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 289931.0938 - mse: 289931.0938\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 950us/step - loss: 280519.2812 - mse: 280519.2812\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 677us/step - loss: 271252.9062 - mse: 271252.9062\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 785us/step - loss: 262174.0000 - mse: 262174.0000\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 950us/step - loss: 253062.4375 - mse: 253062.4375\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 682us/step - loss: 244119.6875 - mse: 244119.6875\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 801us/step - loss: 235474.2344 - mse: 235474.2344\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 797us/step - loss: 227080.0000 - mse: 227080.0000\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 889us/step - loss: 218808.3281 - mse: 218808.3281\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 758us/step - loss: 210681.6875 - mse: 210681.6875\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 202800.2344 - mse: 202800.2344\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 955us/step - loss: 194982.0000 - mse: 194982.0000\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 779us/step - loss: 187412.9062 - mse: 187412.9062\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 880us/step - loss: 180096.4062 - mse: 180096.4062\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 172886.2500 - mse: 172886.2500\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 165889.3750 - mse: 165889.3750\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 159058.5469 - mse: 159058.5469\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 967us/step - loss: 152267.7656 - mse: 152267.7656\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 145713.3750 - mse: 145713.3750\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 139385.9531 - mse: 139385.9531\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 875us/step - loss: 133243.5781 - mse: 133243.5781\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 871us/step - loss: 127243.1875 - mse: 127243.1875\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 866us/step - loss: 121351.2188 - mse: 121351.2188\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 115580.1953 - mse: 115580.1953\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 842us/step - loss: 109909.5312 - mse: 109909.5391\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 713us/step - loss: 104482.9141 - mse: 104482.9141\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 99180.8047 - mse: 99180.8047\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 944us/step - loss: 93998.2969 - mse: 93998.2969\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 88984.2656 - mse: 88984.2656\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 862us/step - loss: 84142.3438 - mse: 84142.3438\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 775us/step - loss: 79469.2578 - mse: 79469.2578\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 748us/step - loss: 74879.9297 - mse: 74879.9297\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 772us/step - loss: 70436.9297 - mse: 70436.9297\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 806us/step - loss: 66122.6172 - mse: 66122.6172\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 880us/step - loss: 62069.6133 - mse: 62069.6133\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 860us/step - loss: 58153.0586 - mse: 58153.0586\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 804us/step - loss: 54465.0742 - mse: 54465.0742\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 866us/step - loss: 50887.1406 - mse: 50887.1406\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 728us/step - loss: 47386.1914 - mse: 47386.1914\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 764us/step - loss: 43998.0352 - mse: 43998.0391\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 890us/step - loss: 40798.5469 - mse: 40798.5469\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 37724.1758 - mse: 37724.1758\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 34803.4414 - mse: 34803.4414\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 32060.9648 - mse: 32060.9648\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 29421.1660 - mse: 29421.1660\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 741us/step - loss: 26873.7344 - mse: 26873.7344\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 772us/step - loss: 24473.9336 - mse: 24473.9336\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 904us/step - loss: 22240.8262 - mse: 22240.8262\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 20148.6953 - mse: 20148.6953\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18202.9785 - mse: 18202.9785\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16333.9102 - mse: 16333.9102\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 883us/step - loss: 14552.1172 - mse: 14552.1172\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 925us/step - loss: 12931.6387 - mse: 12931.6387\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 956us/step - loss: 11412.4873 - mse: 11412.4873\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 957us/step - loss: 10013.2041 - mse: 10013.2041\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8710.7744 - mse: 8710.7744\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7504.0952 - mse: 7504.0952\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 998us/step - loss: 6432.1465 - mse: 6432.1465\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5480.4766 - mse: 5480.4766\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 831us/step - loss: 4614.9541 - mse: 4614.9541\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 953us/step - loss: 3848.4387 - mse: 3848.4387\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3178.2715 - mse: 3178.2715\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 821us/step - loss: 2587.3567 - mse: 2587.3567\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2077.0447 - mse: 2077.0447\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1637.7279 - mse: 1637.7279\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1274.2000 - mse: 1274.2000\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 970us/step - loss: 973.1483 - mse: 973.1483\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 848us/step - loss: 725.3483 - mse: 725.3483\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 986us/step - loss: 528.3386 - mse: 528.3387\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 372.2478 - mse: 372.2478\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 258.6428 - mse: 258.6428\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 813us/step - loss: 179.2568 - mse: 179.2568\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 121.1679 - mse: 121.1679\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 910us/step - loss: 79.7635 - mse: 79.7635\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 55.9276 - mse: 55.9276\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 42.0750 - mse: 42.0750\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 888us/step - loss: 34.9198 - mse: 34.9198\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 31.3429 - mse: 31.3429\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 29.4559 - mse: 29.4559\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 726us/step - loss: 28.7296 - mse: 28.7296\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 753us/step - loss: 28.7014 - mse: 28.7014\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 858us/step - loss: 29.0237 - mse: 29.0237\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 698us/step - loss: 28.6025 - mse: 28.6025\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.6403 - mse: 28.6403\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 29.0391 - mse: 29.0391\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 29.3839 - mse: 29.3839\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 29.6929 - mse: 29.6929\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.5994 - mse: 28.5994\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 27.8390 - mse: 27.8390\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.9253 - mse: 28.9253\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.2304 - mse: 28.2304\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 950us/step - loss: 27.5450 - mse: 27.5450\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 976us/step - loss: 27.5505 - mse: 27.5505\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 849us/step - loss: 27.1479 - mse: 27.1479\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.5188 - mse: 28.5188\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 890us/step - loss: 27.8093 - mse: 27.8093\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 921us/step - loss: 29.2762 - mse: 29.2762\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 782us/step - loss: 38.7580 - mse: 38.7580\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 822us/step - loss: 29.3381 - mse: 29.3381\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 885us/step - loss: 29.7208 - mse: 29.7208\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 25.6830 - mse: 25.6830\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 873us/step - loss: 28.7088 - mse: 28.7088\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 716us/step - loss: 27.1894 - mse: 27.1894\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 734us/step - loss: 26.2574 - mse: 26.2574\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 926us/step - loss: 25.8345 - mse: 25.8345\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 906us/step - loss: 26.6926 - mse: 26.6926\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 836us/step - loss: 34.9986 - mse: 34.9986\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 35.3932 - mse: 35.3932\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 25.8643 - mse: 25.8643\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.3095 - mse: 23.3095\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 907us/step - loss: 22.9087 - mse: 22.9087\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 942us/step - loss: 26.2594 - mse: 26.2594\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 840us/step - loss: 27.6507 - mse: 27.6507\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 902us/step - loss: 28.8810 - mse: 28.8810\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 788us/step - loss: 33.0687 - mse: 33.0687\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 759us/step - loss: 24.6536 - mse: 24.6536\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 762us/step - loss: 21.5548 - mse: 21.5548\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 811us/step - loss: 23.3624 - mse: 23.3624\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.5721 - mse: 30.5721\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 854us/step - loss: 49.6861 - mse: 49.6861\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 829us/step - loss: 30.3946 - mse: 30.3946\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 706us/step - loss: 21.1616 - mse: 21.1616\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 931us/step - loss: 21.0844 - mse: 21.0844\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 838us/step - loss: 20.6769 - mse: 20.6769\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 833us/step - loss: 19.9172 - mse: 19.9172\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 908us/step - loss: 19.7146 - mse: 19.7146\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 767us/step - loss: 32.0517 - mse: 32.0517\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 33.6727 - mse: 33.6727\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.2181 - mse: 19.2181\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 690us/step - loss: 27.2154 - mse: 27.2154\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.1541 - mse: 19.1541\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 737us/step - loss: 25.3969 - mse: 25.3969\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.4487 - mse: 28.4487\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 31.7414 - mse: 31.7414\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 758us/step - loss: 25.0571 - mse: 25.0571\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 873us/step - loss: 18.6270 - mse: 18.6270\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 25.3146 - mse: 25.3146\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 705us/step - loss: 22.5896 - mse: 22.5896\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 707us/step - loss: 17.3161 - mse: 17.3161\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 944us/step - loss: 17.6609 - mse: 17.6609\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 824us/step - loss: 17.1731 - mse: 17.1731\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.5344 - mse: 30.5344\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 742us/step - loss: 25.4096 - mse: 25.4096\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 936us/step - loss: 18.9688 - mse: 18.9688\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 883us/step - loss: 17.2951 - mse: 17.2951\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 816us/step - loss: 18.6896 - mse: 18.6896\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24.2845 - mse: 24.2845\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 817us/step - loss: 23.0724 - mse: 23.0724\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 826us/step - loss: 18.2371 - mse: 18.2371\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 21.9849 - mse: 21.9849\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 861us/step - loss: 35.4852 - mse: 35.4852\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.3040 - mse: 23.3040\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 25.4862 - mse: 25.4862\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 820us/step - loss: 15.2624 - mse: 15.2624\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 800us/step - loss: 17.8056 - mse: 17.8056\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 748us/step - loss: 15.1856 - mse: 15.1856\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 741us/step - loss: 14.3562 - mse: 14.3562\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 852us/step - loss: 29.0524 - mse: 29.0524\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 38.5184 - mse: 38.5184\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.8378 - mse: 14.8378\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.0041 - mse: 17.0041\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.3312 - mse: 16.3312\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 934us/step - loss: 13.6157 - mse: 13.6157\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.5905 - mse: 13.5905\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 859us/step - loss: 14.1196 - mse: 14.1196\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 35.8544 - mse: 35.8544\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.9076 - mse: 19.9076\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.4007 - mse: 13.4007\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.8405 - mse: 12.8405\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 735us/step - loss: 18.5095 - mse: 18.5095\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 836us/step - loss: 21.4293 - mse: 21.4293\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 971us/step - loss: 20.7510 - mse: 20.7510\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 691us/step - loss: 12.3187 - mse: 12.3187\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.3837 - mse: 14.3837\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.7739 - mse: 19.7739\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.7892 - mse: 15.7892\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.2720 - mse: 16.2720\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.6849 - mse: 17.6849\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 790us/step - loss: 13.9635 - mse: 13.9635\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.3892 - mse: 12.3892\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.6676 - mse: 11.6676\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.6640 - mse: 11.6640\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 737us/step - loss: 12.1570 - mse: 12.1570\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 795us/step - loss: 34.8345 - mse: 34.8345\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 803us/step - loss: 27.4561 - mse: 27.4561\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.8794 - mse: 13.8794\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 887us/step - loss: 12.9252 - mse: 12.9252\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.6074 - mse: 10.6074\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 928us/step - loss: 10.4553 - mse: 10.4553\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 830us/step - loss: 13.3647 - mse: 13.3647\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.0744 - mse: 14.0744\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 914us/step - loss: 14.7390 - mse: 14.7390\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 15.7516 - mse: 15.7516\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 773us/step - loss: 18.2646 - mse: 18.2646\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 696us/step - loss: 25.1361 - mse: 25.1361\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 901us/step - loss: 21.3014 - mse: 21.3014\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 965us/step - loss: 12.1042 - mse: 12.1042\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.0259 - mse: 11.0259\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 814us/step - loss: 27.4752 - mse: 27.4752\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 793us/step - loss: 20.0869 - mse: 20.0869\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 958us/step - loss: 10.0517 - mse: 10.0517\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.9996 - mse: 9.9996\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 913us/step - loss: 11.6519 - mse: 11.6519\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 723us/step - loss: 12.8469 - mse: 12.8469\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 795us/step - loss: 22.7331 - mse: 22.7331\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 865us/step - loss: 13.5213 - mse: 13.5213\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.4245 - mse: 10.4245\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.9893 - mse: 19.9893\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.1898 - mse: 14.1898\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.4934 - mse: 9.4934\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4690 - mse: 8.4690\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 906us/step - loss: 14.8304 - mse: 14.8304\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 983us/step - loss: 10.9726 - mse: 10.9726\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 987us/step - loss: 13.0486 - mse: 13.0486\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 20.1765 - mse: 20.1765\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 979us/step - loss: 14.8040 - mse: 14.8040\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 987us/step - loss: 21.2789 - mse: 21.2789\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.2227 - mse: 9.2227\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.8170 - mse: 7.8170\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.2852 - mse: 11.2852\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 27.5013 - mse: 27.5013\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.4542 - mse: 15.4542\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.1216 - mse: 9.1216\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.5140 - mse: 7.5140\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.4319 - mse: 7.4319\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4426 - mse: 8.4426\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 879us/step - loss: 12.4278 - mse: 12.4278\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 825us/step - loss: 15.6306 - mse: 15.6306\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.9269 - mse: 10.9269\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 839us/step - loss: 9.3580 - mse: 9.3580\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.7463 - mse: 15.7463\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.6631 - mse: 12.6631\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.1007 - mse: 7.1007\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.5563 - mse: 7.5563\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18.9797 - mse: 18.9797\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 21.9109 - mse: 21.9109\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 830us/step - loss: 10.3538 - mse: 10.3538\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.6318 - mse: 8.6318\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 920us/step - loss: 10.0135 - mse: 10.0135\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.4176 - mse: 6.4176\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.9830 - mse: 7.9830\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.2383 - mse: 14.2383\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 906us/step - loss: 17.9166 - mse: 17.9166\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 757us/step - loss: 11.6788 - mse: 11.6788\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 929us/step - loss: 7.7294 - mse: 7.7294\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.0208 - mse: 6.0208\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 977us/step - loss: 5.9717 - mse: 5.9717\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 882us/step - loss: 5.8933 - mse: 5.8933\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 973us/step - loss: 8.0291 - mse: 8.0291\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 962us/step - loss: 34.8406 - mse: 34.8406\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 991us/step - loss: 17.8493 - mse: 17.8493\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.4668 - mse: 9.4668\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.8403 - mse: 5.8403\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 924us/step - loss: 5.6832 - mse: 5.6832\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 943us/step - loss: 5.6095 - mse: 5.6095\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.6798 - mse: 7.6798\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.8947 - mse: 7.8947\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.9146 - mse: 10.9146\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 922us/step - loss: 22.6374 - mse: 22.6374\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.3118 - mse: 17.3118\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4175 - mse: 8.4175\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.3442 - mse: 5.3442\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 963us/step - loss: 5.3438 - mse: 5.3438\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 777us/step - loss: 5.1555 - mse: 5.1555\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.9255 - mse: 8.9255\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 693us/step - loss: 25.0006 - mse: 25.0006\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 852us/step - loss: 12.5688 - mse: 12.5688\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 784us/step - loss: 6.6068 - mse: 6.6068\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 810us/step - loss: 9.4703 - mse: 9.4703\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 809us/step - loss: 16.5400 - mse: 16.5400\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 724us/step - loss: 12.3370 - mse: 12.3370\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 860us/step - loss: 7.6386 - mse: 7.6386\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 728us/step - loss: 7.6629 - mse: 7.6629\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 924us/step - loss: 6.5719 - mse: 6.5719\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 812us/step - loss: 5.4465 - mse: 5.4465\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 854us/step - loss: 5.8409 - mse: 5.8409\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 749us/step - loss: 19.7755 - mse: 19.7755\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 759us/step - loss: 17.1749 - mse: 17.1749\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 732us/step - loss: 6.0972 - mse: 6.0972\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 707us/step - loss: 5.2002 - mse: 5.2002\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 878us/step - loss: 7.1621 - mse: 7.1621\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 740us/step - loss: 10.2768 - mse: 10.2768\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 859us/step - loss: 14.6765 - mse: 14.6765\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 930us/step - loss: 8.1397 - mse: 8.1397\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.1681 - mse: 5.1681\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 721us/step - loss: 6.9094 - mse: 6.9094\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 795us/step - loss: 22.9198 - mse: 22.9198\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 682us/step - loss: 14.5933 - mse: 14.5933\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 836us/step - loss: 5.5571 - mse: 5.5571\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 857us/step - loss: 6.1202 - mse: 6.1202\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 901us/step - loss: 5.7411 - mse: 5.7411\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 8.4694 - mse: 8.4694\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.2961 - mse: 7.2961\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 867us/step - loss: 6.2758 - mse: 6.2758\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 768us/step - loss: 8.8313 - mse: 8.8313\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 733us/step - loss: 12.2387 - mse: 12.2387\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 851us/step - loss: 16.3099 - mse: 16.3099\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 842us/step - loss: 14.2600 - mse: 14.2600\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 784us/step - loss: 9.5304 - mse: 9.5304\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 722us/step - loss: 6.5182 - mse: 6.5182\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 734us/step - loss: 4.1457 - mse: 4.1457\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 831us/step - loss: 5.4262 - mse: 5.4262\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 725us/step - loss: 13.1301 - mse: 13.1301\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 831us/step - loss: 15.7165 - mse: 15.7165\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 920us/step - loss: 12.8179 - mse: 12.8179\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 792us/step - loss: 9.9888 - mse: 9.9888\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 823us/step - loss: 7.6646 - mse: 7.6646\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.7980 - mse: 9.7980\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 819us/step - loss: 7.7120 - mse: 7.7120\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 811us/step - loss: 4.4939 - mse: 4.4939\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 865us/step - loss: 3.6637 - mse: 3.6637\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 714us/step - loss: 3.9797 - mse: 3.9797\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 697us/step - loss: 4.3078 - mse: 4.3078\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 794us/step - loss: 9.1673 - mse: 9.1673\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 721us/step - loss: 23.6785 - mse: 23.6785\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 721us/step - loss: 9.0859 - mse: 9.0859\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 949us/step - loss: 6.2773 - mse: 6.2773\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 918us/step - loss: 3.7580 - mse: 3.7580\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 739us/step - loss: 3.6031 - mse: 3.6031\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 686us/step - loss: 11.1765 - mse: 11.1765\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 775us/step - loss: 14.2530 - mse: 14.2530\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 817us/step - loss: 5.7999 - mse: 5.7999\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 883us/step - loss: 4.2444 - mse: 4.2444\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 717us/step - loss: 3.3379 - mse: 3.3379\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 926us/step - loss: 3.5556 - mse: 3.5556\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.2253 - mse: 10.2253\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.3637 - mse: 30.3637\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 732us/step - loss: 7.5315 - mse: 7.5315\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 737us/step - loss: 3.6344 - mse: 3.6344\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 769us/step - loss: 3.2310 - mse: 3.2310\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 839us/step - loss: 3.2204 - mse: 3.2204\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 884us/step - loss: 3.7322 - mse: 3.7322\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 788us/step - loss: 5.4998 - mse: 5.4998\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 913us/step - loss: 15.1298 - mse: 15.1298\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 802us/step - loss: 17.1229 - mse: 17.1229\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 884us/step - loss: 8.8148 - mse: 8.8148\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 829us/step - loss: 5.9170 - mse: 5.9170\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 776us/step - loss: 6.4179 - mse: 6.4179\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 741us/step - loss: 5.8801 - mse: 5.8801\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 811us/step - loss: 4.0179 - mse: 4.0179\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 865us/step - loss: 3.8817 - mse: 3.8817\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 728us/step - loss: 3.6346 - mse: 3.6346\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 933us/step - loss: 10.9124 - mse: 10.9124\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 897us/step - loss: 23.7957 - mse: 23.7957\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.8671 - mse: 10.8671\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.6911 - mse: 4.6911\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 843us/step - loss: 3.1297 - mse: 3.1297\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 736us/step - loss: 5.7778 - mse: 5.7778\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 928us/step - loss: 7.9909 - mse: 7.9909\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.9987 - mse: 10.9987\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 918us/step - loss: 15.4364 - mse: 15.4364\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.1495 - mse: 8.1495\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.9915 - mse: 2.9915\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 838us/step - loss: 2.8548 - mse: 2.8548\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.8648 - mse: 3.8648\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.8177 - mse: 17.8177\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 836us/step - loss: 11.7522 - mse: 11.7522\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.3787 - mse: 4.3787\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 973us/step - loss: 3.6417 - mse: 3.6417\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.5885 - mse: 4.5885\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.0569 - mse: 3.0569\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.6768 - mse: 2.6768\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 955us/step - loss: 5.2783 - mse: 5.2783\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 822us/step - loss: 18.1138 - mse: 18.1138\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.2032 - mse: 15.2032\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 779us/step - loss: 7.0824 - mse: 7.0824\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.6044 - mse: 5.6044\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 790us/step - loss: 7.6824 - mse: 7.6824\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.4313 - mse: 6.4313\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 785us/step - loss: 3.1838 - mse: 3.1838\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.2822 - mse: 4.2822\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.5034 - mse: 12.5034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa167678a00>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.7973183 , 1.218394  , 0.7991186 ],\n",
       "        [0.83894646, 1.1768169 , 0.8407604 ],\n",
       "        [0.818045  , 1.1976866 , 0.8198475 ],\n",
       "        [0.8251872 , 1.1905754 , 0.82700115],\n",
       "        [0.6219785 , 1.3943486 , 0.6239535 ],\n",
       "        [0.6468203 , 1.3693433 , 0.64874506]], dtype=float32),\n",
       " array([-0.2934697 ,  0.3094739 , -0.29159212], dtype=float32),\n",
       " array([[ 0.6904734 ],\n",
       "        [-0.49006087],\n",
       "        [ 0.39967656]], dtype=float32),\n",
       " array([-0.29711878], dtype=float32)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa165e78b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14.964859]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_ones_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.951683</td>\n",
       "      <td>14.964859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.089252</td>\n",
       "      <td>13.107574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>17.327856</td>\n",
       "      <td>13.021454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>21.157095</td>\n",
       "      <td>16.737915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.099650</td>\n",
       "      <td>10.074615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_ones_after_fit\n",
       "abbrev                                                  \n",
       "AL       18.8             17.951683            14.964859\n",
       "AK       18.1             17.089252            13.107574\n",
       "AZ       18.6             17.327856            13.021454\n",
       "AR       22.4             21.157095            16.737915\n",
       "CA       12.0             12.099650            10.074615"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_ones_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.014535574414353"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_ones_after_fit)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to `glorot_uniform` (default)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa167671b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-106.59925]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.20189166,  0.74968207, -0.32342973],\n",
       "        [-0.2794453 , -0.37392366, -0.48311195],\n",
       "        [ 0.78452885, -0.5581418 , -0.5464616 ],\n",
       "        [ 0.59234226,  0.71697783, -0.17473865],\n",
       "        [-0.23720038,  0.2725823 ,  0.02228737],\n",
       "        [ 0.5232624 , -0.58857024, -0.16479516]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[ 0.7873794 ],\n",
       "        [-0.34254897],\n",
       "        [-0.31137425]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-106.59925]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 983us/step - loss: 27899.3906 - mse: 27899.3906\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24986.4941 - mse: 24986.4941\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 845us/step - loss: 23052.1172 - mse: 23052.1172\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 903us/step - loss: 21500.8730 - mse: 21500.8750\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 873us/step - loss: 20177.4492 - mse: 20177.4492\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 867us/step - loss: 18983.7012 - mse: 18983.7012\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 784us/step - loss: 17900.4414 - mse: 17900.4414\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 811us/step - loss: 16870.6406 - mse: 16870.6406\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 922us/step - loss: 15921.2168 - mse: 15921.2168\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 820us/step - loss: 15047.4434 - mse: 15047.4434\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 756us/step - loss: 14234.3271 - mse: 14234.3271\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 776us/step - loss: 13473.5205 - mse: 13473.5205\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 717us/step - loss: 12729.5654 - mse: 12729.5654\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 777us/step - loss: 11998.4346 - mse: 11998.4346\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 912us/step - loss: 11291.8252 - mse: 11291.8252\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 808us/step - loss: 10613.1641 - mse: 10613.1641\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 808us/step - loss: 9981.2295 - mse: 9981.2295\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 871us/step - loss: 9380.2910 - mse: 9380.2910\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 800us/step - loss: 8794.7246 - mse: 8794.7246\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 785us/step - loss: 8241.5479 - mse: 8241.5479\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 943us/step - loss: 7708.7085 - mse: 7708.7085\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 861us/step - loss: 7200.1763 - mse: 7200.1763\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 836us/step - loss: 6701.0269 - mse: 6701.0269\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 794us/step - loss: 6225.6299 - mse: 6225.6299\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 813us/step - loss: 5775.9082 - mse: 5775.9082\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5350.7573 - mse: 5350.7573\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 854us/step - loss: 4949.1279 - mse: 4949.1279\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 848us/step - loss: 4571.5254 - mse: 4571.5254\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 918us/step - loss: 4208.7231 - mse: 4208.7231\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 911us/step - loss: 3867.8955 - mse: 3867.8955\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 860us/step - loss: 3538.9019 - mse: 3538.9019\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 980us/step - loss: 3217.0901 - mse: 3217.0901\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 869us/step - loss: 2917.6638 - mse: 2917.6638\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 849us/step - loss: 2646.6360 - mse: 2646.6360\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 768us/step - loss: 2392.2798 - mse: 2392.2798\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 811us/step - loss: 2162.1729 - mse: 2162.1729\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1946.1234 - mse: 1946.1234\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 982us/step - loss: 1749.2678 - mse: 1749.2678\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 924us/step - loss: 1569.7207 - mse: 1569.7207\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 777us/step - loss: 1401.2213 - mse: 1401.2213\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 1242.5834 - mse: 1242.5834\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 986us/step - loss: 1103.3344 - mse: 1103.3344\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 846us/step - loss: 978.1772 - mse: 978.1772\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 850us/step - loss: 864.0790 - mse: 864.0790\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 929us/step - loss: 760.8723 - mse: 760.8723\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 803us/step - loss: 676.9199 - mse: 676.9199\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 784us/step - loss: 603.3459 - mse: 603.3459\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 539.6411 - mse: 539.6411\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 483.6437 - mse: 483.6437\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 440.6700 - mse: 440.6700\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 403.5744 - mse: 403.5744\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 372.2879 - mse: 372.2879\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 349.6049 - mse: 349.6049\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 332.9279 - mse: 332.9279\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 319.4481 - mse: 319.4481\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 822us/step - loss: 308.4062 - mse: 308.4062\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 299.8192 - mse: 299.8192\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 298.4134 - mse: 298.4134\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 759us/step - loss: 292.3600 - mse: 292.3600\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 290.5640 - mse: 290.5640\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 285.0113 - mse: 285.0113\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 961us/step - loss: 285.4225 - mse: 285.4225\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 281.4666 - mse: 281.4666\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 281.5101 - mse: 281.5101\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 887us/step - loss: 278.4105 - mse: 278.4105\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 962us/step - loss: 274.8349 - mse: 274.8349\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 271.8079 - mse: 271.8079\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 272.0241 - mse: 272.0241\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 271.8734 - mse: 271.8734\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 264.1957 - mse: 264.1957\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 263.2626 - mse: 263.2626\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 258.8475 - mse: 258.8475\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 718us/step - loss: 257.0672 - mse: 257.0672\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 803us/step - loss: 253.7437 - mse: 253.7437\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 809us/step - loss: 250.1035 - mse: 250.1035\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 251.3690 - mse: 251.3690\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 918us/step - loss: 257.0573 - mse: 257.0573\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 244.5295 - mse: 244.5295\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 240.8991 - mse: 240.8991\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 238.8071 - mse: 238.8071\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 946us/step - loss: 236.7909 - mse: 236.7909\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 231.5953 - mse: 231.5953\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 228.5687 - mse: 228.5687\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 784us/step - loss: 226.2759 - mse: 226.2759\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 947us/step - loss: 222.9595 - mse: 222.9595\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 885us/step - loss: 220.3726 - mse: 220.3726\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 849us/step - loss: 223.8223 - mse: 223.8223\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 215.4729 - mse: 215.4729\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 210.9844 - mse: 210.9844\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 211.0529 - mse: 211.0529\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 211.9912 - mse: 211.9912\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 204.4083 - mse: 204.4083\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 203.6322 - mse: 203.6322\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 872us/step - loss: 202.3936 - mse: 202.3936\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 196.4580 - mse: 196.4580\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 194.0032 - mse: 194.0032\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 191.3598 - mse: 191.3598\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 188.6554 - mse: 188.6554\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 186.1597 - mse: 186.1597\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 182.9630 - mse: 182.9630\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 180.4422 - mse: 180.4422\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 896us/step - loss: 177.6016 - mse: 177.6016\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 177.2043 - mse: 177.2043\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 984us/step - loss: 173.3932 - mse: 173.3932\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 910us/step - loss: 174.9198 - mse: 174.9198\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 169.0491 - mse: 169.0491\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 175.1539 - mse: 175.1539\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 165.2986 - mse: 165.2986\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 162.1634 - mse: 162.1634\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 930us/step - loss: 159.6207 - mse: 159.6207\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 957us/step - loss: 156.5002 - mse: 156.5002\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 154.6294 - mse: 154.6294\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 152.7999 - mse: 152.7999\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 937us/step - loss: 153.9086 - mse: 153.9086\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 949us/step - loss: 148.8581 - mse: 148.8581\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 145.3961 - mse: 145.3961\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 787us/step - loss: 143.7138 - mse: 143.7138\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 853us/step - loss: 141.3235 - mse: 141.3235\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 138.6577 - mse: 138.6577\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 137.4967 - mse: 137.4967\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 135.2269 - mse: 135.2269\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 131.8579 - mse: 131.8579\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 136.0104 - mse: 136.0104\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 128.8886 - mse: 128.8886\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 126.3671 - mse: 126.3671\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 974us/step - loss: 128.4394 - mse: 128.4394\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 123.4803 - mse: 123.4803\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 122.0611 - mse: 122.0611\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 875us/step - loss: 119.3342 - mse: 119.3342\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 885us/step - loss: 120.9104 - mse: 120.9104\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 936us/step - loss: 116.4961 - mse: 116.4961\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 753us/step - loss: 113.8181 - mse: 113.8181\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 811us/step - loss: 112.3441 - mse: 112.3441\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 115.5034 - mse: 115.5034\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 109.9340 - mse: 109.9340\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 107.2911 - mse: 107.2911\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 847us/step - loss: 105.5759 - mse: 105.5759\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 871us/step - loss: 104.8795 - mse: 104.8795\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 107.2349 - mse: 107.2349\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 102.2495 - mse: 102.2495\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 782us/step - loss: 100.3371 - mse: 100.3371\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 741us/step - loss: 98.4821 - mse: 98.4821\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 842us/step - loss: 100.2899 - mse: 100.2899\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 683us/step - loss: 96.4360 - mse: 96.4360\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 927us/step - loss: 93.7314 - mse: 93.7314\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 839us/step - loss: 93.0575 - mse: 93.0575\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 851us/step - loss: 92.7408 - mse: 92.7408\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 780us/step - loss: 93.1178 - mse: 93.1178\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 810us/step - loss: 88.8919 - mse: 88.8919\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 894us/step - loss: 86.6507 - mse: 86.6507\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 748us/step - loss: 85.3209 - mse: 85.3209\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 813us/step - loss: 83.8680 - mse: 83.8680\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 739us/step - loss: 83.7919 - mse: 83.7919\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 993us/step - loss: 83.2566 - mse: 83.2566\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 81.7170 - mse: 81.7170\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 79.1443 - mse: 79.1443\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 79.3267 - mse: 79.3267\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 76.8859 - mse: 76.8859\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 75.5445 - mse: 75.5445\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 75.6793 - mse: 75.6793\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 988us/step - loss: 73.1831 - mse: 73.1831\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 998us/step - loss: 74.1243 - mse: 74.1243\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 71.0320 - mse: 71.0320\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 69.4903 - mse: 69.4903\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 68.5741 - mse: 68.5741\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 840us/step - loss: 67.2063 - mse: 67.2063\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 806us/step - loss: 66.7657 - mse: 66.7657\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 914us/step - loss: 66.0529 - mse: 66.0529\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 757us/step - loss: 64.4330 - mse: 64.4330\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 890us/step - loss: 62.9991 - mse: 62.9991\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 61.8308 - mse: 61.8308\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 62.8440 - mse: 62.8440\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 955us/step - loss: 60.1521 - mse: 60.1521\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 58.5124 - mse: 58.5124\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 58.5950 - mse: 58.5950\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 909us/step - loss: 57.1346 - mse: 57.1346\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 855us/step - loss: 55.8970 - mse: 55.8970\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 812us/step - loss: 60.7326 - mse: 60.7326\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 762us/step - loss: 56.1720 - mse: 56.1720\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 962us/step - loss: 53.7853 - mse: 53.7853\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 838us/step - loss: 52.4875 - mse: 52.4875\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 908us/step - loss: 51.6914 - mse: 51.6914\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 900us/step - loss: 51.3592 - mse: 51.3592\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 778us/step - loss: 50.1944 - mse: 50.1944\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 49.1697 - mse: 49.1697\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 871us/step - loss: 50.8004 - mse: 50.8004\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 48.3428 - mse: 48.3428\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 46.8978 - mse: 46.8978\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 884us/step - loss: 46.1073 - mse: 46.1073\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 957us/step - loss: 45.3095 - mse: 45.3095\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 787us/step - loss: 45.7666 - mse: 45.7666\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 746us/step - loss: 45.0899 - mse: 45.0899\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 741us/step - loss: 43.3286 - mse: 43.3286\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 43.4587 - mse: 43.4587\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 42.3125 - mse: 42.3125\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 41.3702 - mse: 41.3702\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 834us/step - loss: 40.6787 - mse: 40.6787\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 40.5516 - mse: 40.5516\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 39.6030 - mse: 39.6030\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 38.8554 - mse: 38.8554\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 40.6682 - mse: 40.6682\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 38.9699 - mse: 38.9699\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 36.7039 - mse: 36.7039\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 35.9711 - mse: 35.9711\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 35.5909 - mse: 35.5909\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 890us/step - loss: 35.6589 - mse: 35.6589\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 34.6838 - mse: 34.6838\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 34.0036 - mse: 34.0036\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 952us/step - loss: 33.1938 - mse: 33.1938\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 32.5893 - mse: 32.5893\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 31.7597 - mse: 31.7597\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 970us/step - loss: 34.2868 - mse: 34.2868\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 33.0025 - mse: 33.0025\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 32.4030 - mse: 32.4030\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 949us/step - loss: 30.7283 - mse: 30.7283\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 841us/step - loss: 29.6716 - mse: 29.6716\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 886us/step - loss: 29.0999 - mse: 29.0999\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 803us/step - loss: 28.8728 - mse: 28.8728\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 818us/step - loss: 28.6238 - mse: 28.6238\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 826us/step - loss: 28.0830 - mse: 28.0830\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 762us/step - loss: 27.3610 - mse: 27.3610\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 884us/step - loss: 26.9662 - mse: 26.9662\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 842us/step - loss: 26.8765 - mse: 26.8765\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 818us/step - loss: 26.6484 - mse: 26.6484\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 25.6842 - mse: 25.6842\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 26.2708 - mse: 26.2708\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 945us/step - loss: 24.9059 - mse: 24.9059\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 26.1412 - mse: 26.1412\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24.5509 - mse: 24.5509\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.3291 - mse: 23.3291\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 23.0000 - mse: 23.0000\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.7631 - mse: 22.7631\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 823us/step - loss: 22.6714 - mse: 22.6714\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 936us/step - loss: 22.6116 - mse: 22.6116\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 793us/step - loss: 21.5556 - mse: 21.5556\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 816us/step - loss: 21.1051 - mse: 21.1051\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 771us/step - loss: 20.7762 - mse: 20.7762\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 957us/step - loss: 21.0433 - mse: 21.0433\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 932us/step - loss: 20.3750 - mse: 20.3750\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 20.2089 - mse: 20.2089\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.6224 - mse: 19.6224\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18.9951 - mse: 18.9951\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 916us/step - loss: 19.1462 - mse: 19.1462\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 18.8729 - mse: 18.8729\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18.7959 - mse: 18.7959\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 966us/step - loss: 18.9457 - mse: 18.9457\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 17.9792 - mse: 17.9792\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.2377 - mse: 17.2377\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.3809 - mse: 17.3809\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18.1126 - mse: 18.1126\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 995us/step - loss: 18.5080 - mse: 18.5080\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.7722 - mse: 16.7722\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 855us/step - loss: 16.6419 - mse: 16.6419\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 936us/step - loss: 15.8639 - mse: 15.8639\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 876us/step - loss: 17.2082 - mse: 17.2082\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.1558 - mse: 16.1558\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 925us/step - loss: 15.3260 - mse: 15.3260\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 745us/step - loss: 15.0169 - mse: 15.0169\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 893us/step - loss: 15.0803 - mse: 15.0803\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 928us/step - loss: 14.8179 - mse: 14.8179\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 974us/step - loss: 14.3228 - mse: 14.3228\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.6291 - mse: 14.6291\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 944us/step - loss: 14.7646 - mse: 14.7646\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 903us/step - loss: 14.1735 - mse: 14.1735\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.8111 - mse: 15.8111\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 987us/step - loss: 13.7728 - mse: 13.7728\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 815us/step - loss: 13.6051 - mse: 13.6051\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.0418 - mse: 13.0418\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.9237 - mse: 12.9237\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.3363 - mse: 13.3363\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 966us/step - loss: 13.7195 - mse: 13.7195\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.4431 - mse: 12.4431\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.2106 - mse: 12.2106\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 893us/step - loss: 12.3671 - mse: 12.3671\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.9022 - mse: 11.9022\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.9265 - mse: 11.9265\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 776us/step - loss: 11.5685 - mse: 11.5685\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 930us/step - loss: 12.0283 - mse: 12.0283\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.2215 - mse: 12.2215\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.7719 - mse: 12.7719\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 894us/step - loss: 11.4232 - mse: 11.4232\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 755us/step - loss: 11.6127 - mse: 11.6127\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 747us/step - loss: 12.3929 - mse: 12.3929\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.2321 - mse: 11.2321\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.4738 - mse: 10.4738\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 945us/step - loss: 10.6847 - mse: 10.6847\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.5179 - mse: 10.5179\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 958us/step - loss: 10.4570 - mse: 10.4570\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.1642 - mse: 11.1642\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.6791 - mse: 10.6791\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.9473 - mse: 9.9473\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.6793 - mse: 9.6793\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.7820 - mse: 9.7820\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 960us/step - loss: 9.4287 - mse: 9.4287\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 777us/step - loss: 9.3956 - mse: 9.3956\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 785us/step - loss: 10.2598 - mse: 10.2598\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.2803 - mse: 9.2803\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 924us/step - loss: 9.1127 - mse: 9.1127\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.7768 - mse: 9.7768\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.3308 - mse: 9.3308\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 834us/step - loss: 8.6362 - mse: 8.6362\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 832us/step - loss: 8.9776 - mse: 8.9776\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 968us/step - loss: 9.6633 - mse: 9.6633\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 749us/step - loss: 8.6070 - mse: 8.6070\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 950us/step - loss: 8.3102 - mse: 8.3102\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.6783 - mse: 8.6783\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 978us/step - loss: 8.1307 - mse: 8.1307\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.0564 - mse: 8.0564\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 816us/step - loss: 7.8723 - mse: 7.8723\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4306 - mse: 8.4306\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.5274 - mse: 8.5274\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.5397 - mse: 8.5397\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 952us/step - loss: 8.2160 - mse: 8.2160\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.9033 - mse: 7.9033\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.2852 - mse: 8.2852\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 733us/step - loss: 7.6740 - mse: 7.6740\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 813us/step - loss: 7.4958 - mse: 7.4958\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.5850 - mse: 7.5850\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 925us/step - loss: 7.3437 - mse: 7.3437\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.6945 - mse: 7.6945\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.9527 - mse: 6.9527\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.9031 - mse: 6.9031\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.8937 - mse: 7.8937\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 864us/step - loss: 8.0683 - mse: 8.0683\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.9666 - mse: 6.9666\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.6599 - mse: 6.6599\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 929us/step - loss: 6.5874 - mse: 6.5874\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.5452 - mse: 6.5452\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.2110 - mse: 7.2110\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.6301 - mse: 6.6301\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.0467 - mse: 7.0467\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.2449 - mse: 6.2449\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.3082 - mse: 6.3082\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.2581 - mse: 6.2581\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.0923 - mse: 6.0923\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.9833 - mse: 5.9833\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.1131 - mse: 6.1131\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.8941 - mse: 5.8941\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.9708 - mse: 7.9708\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.6237 - mse: 6.6237\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 866us/step - loss: 5.7619 - mse: 5.7619\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 920us/step - loss: 5.7510 - mse: 5.7510\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.7007 - mse: 5.7007\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.5123 - mse: 5.5123\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.7243 - mse: 5.7243\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 792us/step - loss: 5.7797 - mse: 5.7797\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 978us/step - loss: 5.8138 - mse: 5.8138\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 895us/step - loss: 6.2104 - mse: 6.2104\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.8488 - mse: 5.8488\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.3831 - mse: 5.3831\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 751us/step - loss: 5.3971 - mse: 5.3971\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 883us/step - loss: 5.4391 - mse: 5.4391\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 773us/step - loss: 5.0900 - mse: 5.0900\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 759us/step - loss: 5.3975 - mse: 5.3975\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 783us/step - loss: 6.4947 - mse: 6.4947\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 772us/step - loss: 5.0021 - mse: 5.0021\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 770us/step - loss: 4.8746 - mse: 4.8746\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 885us/step - loss: 4.9900 - mse: 4.9900\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.5535 - mse: 6.5535\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.4809 - mse: 5.4809\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.7856 - mse: 4.7856\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.1583 - mse: 5.1583\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 5.5648 - mse: 5.5648\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.7120 - mse: 4.7120\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.2093 - mse: 5.2093\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.6526 - mse: 4.6526\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 952us/step - loss: 4.7547 - mse: 4.7547\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 738us/step - loss: 5.2191 - mse: 5.2191\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 740us/step - loss: 4.7271 - mse: 4.7271\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.4808 - mse: 4.4808\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 815us/step - loss: 5.4214 - mse: 5.4214\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.5853 - mse: 4.5853\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.5810 - mse: 4.5810\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.9158 - mse: 4.9158\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 849us/step - loss: 4.3804 - mse: 4.3804\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 786us/step - loss: 4.8722 - mse: 4.8722\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 989us/step - loss: 4.7796 - mse: 4.7796\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 904us/step - loss: 4.6038 - mse: 4.6038\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 820us/step - loss: 4.1521 - mse: 4.1521\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 875us/step - loss: 4.1309 - mse: 4.1309\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 833us/step - loss: 4.2136 - mse: 4.2136\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.1258 - mse: 4.1258\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.9799 - mse: 3.9799\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.1626 - mse: 4.1626\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.4909 - mse: 5.4909\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.1005 - mse: 4.1005\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 899us/step - loss: 4.2196 - mse: 4.2196\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.9989 - mse: 3.9989\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 970us/step - loss: 4.3221 - mse: 4.3221\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.7728 - mse: 3.7728\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.1730 - mse: 4.1730\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.4816 - mse: 4.4816\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.7138 - mse: 3.7138\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.1132 - mse: 4.1132\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 996us/step - loss: 3.8172 - mse: 3.8172\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.8976 - mse: 4.8976\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 802us/step - loss: 3.8313 - mse: 3.8313\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 849us/step - loss: 3.5543 - mse: 3.5543\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 955us/step - loss: 3.8341 - mse: 3.8341\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 861us/step - loss: 3.5794 - mse: 3.5794\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.5109 - mse: 3.5109\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 817us/step - loss: 3.4287 - mse: 3.4287\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 805us/step - loss: 3.4439 - mse: 3.4439\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.3468 - mse: 3.3468\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 834us/step - loss: 3.8564 - mse: 3.8564\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 787us/step - loss: 3.3144 - mse: 3.3144\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 759us/step - loss: 4.6979 - mse: 4.6979\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.0157 - mse: 4.0157\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.4033 - mse: 3.4033\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 873us/step - loss: 3.6553 - mse: 3.6553\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 889us/step - loss: 3.4132 - mse: 3.4132\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 879us/step - loss: 3.1797 - mse: 3.1797\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 802us/step - loss: 3.2311 - mse: 3.2311\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 986us/step - loss: 3.1133 - mse: 3.1133\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 888us/step - loss: 3.5198 - mse: 3.5198\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.9902 - mse: 3.9902\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 833us/step - loss: 3.0648 - mse: 3.0648\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 756us/step - loss: 3.0290 - mse: 3.0290\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 959us/step - loss: 3.7587 - mse: 3.7587\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 998us/step - loss: 3.6088 - mse: 3.6088\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 920us/step - loss: 2.9583 - mse: 2.9583\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 939us/step - loss: 3.0359 - mse: 3.0359\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.4944 - mse: 3.4944\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.0551 - mse: 3.0551\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.1066 - mse: 3.1066\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 924us/step - loss: 3.0674 - mse: 3.0674\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 848us/step - loss: 4.9613 - mse: 4.9613\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 829us/step - loss: 2.9297 - mse: 2.9297\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 901us/step - loss: 2.7983 - mse: 2.7983\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 926us/step - loss: 2.9219 - mse: 2.9219\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 810us/step - loss: 2.9001 - mse: 2.9001\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 867us/step - loss: 2.8704 - mse: 2.8704\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 918us/step - loss: 2.7539 - mse: 2.7539\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 849us/step - loss: 2.7829 - mse: 2.7829\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 813us/step - loss: 3.2248 - mse: 3.2248\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6858 - mse: 2.6858\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 946us/step - loss: 3.8447 - mse: 3.8447\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 947us/step - loss: 2.7928 - mse: 2.7928\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 710us/step - loss: 3.0860 - mse: 3.0860\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 792us/step - loss: 3.2750 - mse: 3.2750\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 731us/step - loss: 2.6900 - mse: 2.6900\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 797us/step - loss: 2.6735 - mse: 2.6735\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 798us/step - loss: 3.5058 - mse: 3.5058\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.4352 - mse: 3.4352\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.5767 - mse: 2.5767\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 780us/step - loss: 2.5667 - mse: 2.5667\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 750us/step - loss: 2.5378 - mse: 2.5378\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 907us/step - loss: 2.5204 - mse: 2.5204\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.8801 - mse: 2.8801\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.9224 - mse: 2.9224\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 744us/step - loss: 2.7571 - mse: 2.7571\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 896us/step - loss: 2.5067 - mse: 2.5067\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4844 - mse: 2.4844\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 781us/step - loss: 2.4880 - mse: 2.4880\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 974us/step - loss: 2.7095 - mse: 2.7095\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 980us/step - loss: 4.2274 - mse: 4.2274\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 925us/step - loss: 2.7519 - mse: 2.7519\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.4047 - mse: 2.4047\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.4101 - mse: 2.4101\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.3429 - mse: 2.3429\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.0480 - mse: 3.0480\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0296 - mse: 3.0296\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 934us/step - loss: 2.3733 - mse: 2.3733\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 844us/step - loss: 2.7260 - mse: 2.7260\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.3377 - mse: 2.3377\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 911us/step - loss: 2.4082 - mse: 2.4082\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8433 - mse: 2.8433\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 753us/step - loss: 2.6959 - mse: 2.6959\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 863us/step - loss: 2.3165 - mse: 2.3165\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 931us/step - loss: 3.0235 - mse: 3.0235\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 801us/step - loss: 2.5048 - mse: 2.5048\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.2239 - mse: 2.2239\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.5281 - mse: 2.5281\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.4720 - mse: 2.4720\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.5529 - mse: 2.5529\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.4789 - mse: 2.4789\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 703us/step - loss: 2.1796 - mse: 2.1796\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 707us/step - loss: 2.1910 - mse: 2.1910\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 712us/step - loss: 3.0107 - mse: 3.0107\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 819us/step - loss: 2.6266 - mse: 2.6266\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.3100 - mse: 2.3100\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.0805 - mse: 3.0805\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.1589 - mse: 2.1589\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 744us/step - loss: 2.3874 - mse: 2.3874\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.2628 - mse: 2.2628\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.1069 - mse: 2.1069\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.1744 - mse: 2.1744\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 900us/step - loss: 2.7929 - mse: 2.7929\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 849us/step - loss: 2.3050 - mse: 2.3050\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.1749 - mse: 2.1749\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 778us/step - loss: 2.7621 - mse: 2.7621\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.0888 - mse: 2.0888\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 724us/step - loss: 2.0458 - mse: 2.0458\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 874us/step - loss: 2.0654 - mse: 2.0654\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 867us/step - loss: 2.2948 - mse: 2.2948\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 819us/step - loss: 2.8562 - mse: 2.8562\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 944us/step - loss: 2.1697 - mse: 2.1697\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 978us/step - loss: 2.0480 - mse: 2.0480\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 705us/step - loss: 2.8202 - mse: 2.8202\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 769us/step - loss: 2.2471 - mse: 2.2471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa1678cadf0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.24518865,  0.8406646 , -0.36773333],\n",
       "        [-0.13619655, -0.21060237, -0.6280734 ],\n",
       "        [ 0.76239794, -0.40776572, -0.52458113],\n",
       "        [ 0.68336594,  0.89719933, -0.2669125 ],\n",
       "        [-0.13340119,  0.14749989, -0.08402763],\n",
       "        [ 0.2905875 , -0.5260684 ,  0.06857969]], dtype=float32),\n",
       " array([-0.01607909,  0.05470397,  0.01541677], dtype=float32),\n",
       " array([[ 0.3249336 ],\n",
       "        [ 0.0654436 ],\n",
       "        [-0.37167722]], dtype=float32),\n",
       " array([-0.01770924], dtype=float32)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa166d07d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[19.250841]], dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_ones_after_fit</th>\n",
       "      <th>pred_glorot_uniform_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.951683</td>\n",
       "      <td>14.964859</td>\n",
       "      <td>19.250841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.089252</td>\n",
       "      <td>13.107574</td>\n",
       "      <td>18.051170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>17.327856</td>\n",
       "      <td>13.021454</td>\n",
       "      <td>17.532259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>21.157095</td>\n",
       "      <td>16.737915</td>\n",
       "      <td>21.811823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.099650</td>\n",
       "      <td>10.074615</td>\n",
       "      <td>13.856906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_ones_after_fit  \\\n",
       "abbrev                                                     \n",
       "AL       18.8             17.951683            14.964859   \n",
       "AK       18.1             17.089252            13.107574   \n",
       "AZ       18.6             17.327856            13.021454   \n",
       "AR       22.4             21.157095            16.737915   \n",
       "CA       12.0             12.099650            10.074615   \n",
       "\n",
       "        pred_glorot_uniform_after_fit  \n",
       "abbrev                                 \n",
       "AL                          19.250841  \n",
       "AK                          18.051170  \n",
       "AZ                          17.532259  \n",
       "AR                          21.811823  \n",
       "CA                          13.856906  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_glorot_uniform_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0016545770185608"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_glorot_uniform_after_fit)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with the Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=558\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=558\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `sigmoid` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa16711d430>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa165c3a5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_ones_after_fit</th>\n",
       "      <th>pred_glorot_uniform_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.951683</td>\n",
       "      <td>14.964859</td>\n",
       "      <td>19.250841</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.089252</td>\n",
       "      <td>13.107574</td>\n",
       "      <td>18.051170</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>17.327856</td>\n",
       "      <td>13.021454</td>\n",
       "      <td>17.532259</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>21.157095</td>\n",
       "      <td>16.737915</td>\n",
       "      <td>21.811823</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.099650</td>\n",
       "      <td>10.074615</td>\n",
       "      <td>13.856906</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_ones_after_fit  \\\n",
       "abbrev                                                     \n",
       "AL       18.8             17.951683            14.964859   \n",
       "AK       18.1             17.089252            13.107574   \n",
       "AZ       18.6             17.327856            13.021454   \n",
       "AR       22.4             21.157095            16.737915   \n",
       "CA       12.0             12.099650            10.074615   \n",
       "\n",
       "        pred_glorot_uniform_after_fit  pred_sigmoid  \n",
       "abbrev                                               \n",
       "AL                          19.250841           1.0  \n",
       "AK                          18.051170           1.0  \n",
       "AZ                          17.532259           1.0  \n",
       "AR                          21.811823           1.0  \n",
       "CA                          13.856906           1.0  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_sigmoid'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.40764705882347"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.7158463 , -0.78892463,  0.2502904 ],\n",
       "        [ 0.39776313,  0.6903732 , -0.48467788],\n",
       "        [-0.812131  , -0.3465481 ,  0.18164563],\n",
       "        [-0.7716179 ,  0.69405067, -0.38137576],\n",
       "        [ 0.003869  , -0.6757145 , -0.3349928 ],\n",
       "        [ 0.34879982, -0.31880927,  0.28462029]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.12206697],\n",
       "        [-0.8285792 ],\n",
       "        [ 0.8257047 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `linear` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa167d775e0>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa1666b63a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_ones_after_fit</th>\n",
       "      <th>pred_glorot_uniform_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.951683</td>\n",
       "      <td>14.964859</td>\n",
       "      <td>19.250841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.365091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.089252</td>\n",
       "      <td>13.107574</td>\n",
       "      <td>18.051170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.778942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>17.327856</td>\n",
       "      <td>13.021454</td>\n",
       "      <td>17.532259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.798290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>21.157095</td>\n",
       "      <td>16.737915</td>\n",
       "      <td>21.811823</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.024250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.099650</td>\n",
       "      <td>10.074615</td>\n",
       "      <td>13.856906</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.972692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_ones_after_fit  \\\n",
       "abbrev                                                     \n",
       "AL       18.8             17.951683            14.964859   \n",
       "AK       18.1             17.089252            13.107574   \n",
       "AZ       18.6             17.327856            13.021454   \n",
       "AR       22.4             21.157095            16.737915   \n",
       "CA       12.0             12.099650            10.074615   \n",
       "\n",
       "        pred_glorot_uniform_after_fit  pred_sigmoid  pred_linear  \n",
       "abbrev                                                            \n",
       "AL                          19.250841           1.0    20.365091  \n",
       "AK                          18.051170           1.0    20.778942  \n",
       "AZ                          17.532259           1.0    20.798290  \n",
       "AR                          21.811823           1.0    23.024250  \n",
       "CA                          13.856906           1.0    14.972692  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_linear'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.674486751273643"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_linear)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.5014214 , -0.18883574, -0.31095442],\n",
       "        [-0.38498098, -0.34991992, -0.6038031 ],\n",
       "        [-0.36868092,  0.2215054 , -0.4014794 ],\n",
       "        [-0.74584025, -0.14419863,  0.50687015],\n",
       "        [ 0.5455578 , -0.3821236 ,  0.11357681],\n",
       "        [ 0.5760328 , -0.42429483,  0.15869594]], dtype=float32),\n",
       " array([-0.14048861, -0.1438739 , -0.05790894], dtype=float32),\n",
       " array([[-0.6756765],\n",
       "        [-0.9413002],\n",
       "        [ 0.0754413]], dtype=float32),\n",
       " array([0.14215441], dtype=float32)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `tanh` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa168370df0>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa16788b4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_ones_after_fit</th>\n",
       "      <th>pred_glorot_uniform_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.951683</td>\n",
       "      <td>14.964859</td>\n",
       "      <td>19.250841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.365091</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.089252</td>\n",
       "      <td>13.107574</td>\n",
       "      <td>18.051170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.778942</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>17.327856</td>\n",
       "      <td>13.021454</td>\n",
       "      <td>17.532259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.798290</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>21.157095</td>\n",
       "      <td>16.737915</td>\n",
       "      <td>21.811823</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.024250</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.099650</td>\n",
       "      <td>10.074615</td>\n",
       "      <td>13.856906</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.972692</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_ones_after_fit  \\\n",
       "abbrev                                                     \n",
       "AL       18.8             17.951683            14.964859   \n",
       "AK       18.1             17.089252            13.107574   \n",
       "AZ       18.6             17.327856            13.021454   \n",
       "AR       22.4             21.157095            16.737915   \n",
       "CA       12.0             12.099650            10.074615   \n",
       "\n",
       "        pred_glorot_uniform_after_fit  pred_sigmoid  pred_linear  pred_tanh  \n",
       "abbrev                                                                       \n",
       "AL                          19.250841           1.0    20.365091       -1.0  \n",
       "AK                          18.051170           1.0    20.778942       -1.0  \n",
       "AZ                          17.532259           1.0    20.798290       -1.0  \n",
       "AR                          21.811823           1.0    23.024250       -1.0  \n",
       "CA                          13.856906           1.0    14.972692       -1.0  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_tanh'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298.5684313725491"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_tanh)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.4028858 , -0.1838119 ,  0.26676416],\n",
       "        [-0.297382  , -0.597624  ,  0.34437466],\n",
       "        [-0.58686084, -0.10524899, -0.5348896 ],\n",
       "        [-0.755542  ,  0.00188923,  0.62018645],\n",
       "        [ 0.32943916,  0.8102869 , -0.8112488 ],\n",
       "        [ 0.2683066 , -0.28421736,  0.6138747 ]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.01292026],\n",
       "        [ 0.90008485],\n",
       "        [ 1.0890981 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `relu` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa169178eb0>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa1676c9dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_ones_after_fit</th>\n",
       "      <th>pred_glorot_uniform_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "      <th>pred_relu</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.951683</td>\n",
       "      <td>14.964859</td>\n",
       "      <td>19.250841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.365091</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.089252</td>\n",
       "      <td>13.107574</td>\n",
       "      <td>18.051170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.778942</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>17.327856</td>\n",
       "      <td>13.021454</td>\n",
       "      <td>17.532259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.798290</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>21.157095</td>\n",
       "      <td>16.737915</td>\n",
       "      <td>21.811823</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.024250</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.099650</td>\n",
       "      <td>10.074615</td>\n",
       "      <td>13.856906</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.972692</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_ones_after_fit  \\\n",
       "abbrev                                                     \n",
       "AL       18.8             17.951683            14.964859   \n",
       "AK       18.1             17.089252            13.107574   \n",
       "AZ       18.6             17.327856            13.021454   \n",
       "AR       22.4             21.157095            16.737915   \n",
       "CA       12.0             12.099650            10.074615   \n",
       "\n",
       "        pred_glorot_uniform_after_fit  pred_sigmoid  pred_linear  pred_tanh  \\\n",
       "abbrev                                                                        \n",
       "AL                          19.250841           1.0    20.365091       -1.0   \n",
       "AK                          18.051170           1.0    20.778942       -1.0   \n",
       "AZ                          17.532259           1.0    20.798290       -1.0   \n",
       "AR                          21.811823           1.0    23.024250       -1.0   \n",
       "CA                          13.856906           1.0    14.972692       -1.0   \n",
       "\n",
       "        pred_relu  \n",
       "abbrev             \n",
       "AL            0.0  \n",
       "AK            0.0  \n",
       "AZ            0.0  \n",
       "AR            0.0  \n",
       "CA            0.0  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_relu'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265.98803921568634"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_relu)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.15148377,  0.42218828, -0.13979065],\n",
       "        [-0.47028744, -0.371429  , -0.0369879 ],\n",
       "        [-0.6061427 ,  0.3815949 , -0.34527323],\n",
       "        [ 0.15844125, -0.5183379 , -0.18875414],\n",
       "        [ 0.31505573, -0.7009627 , -0.73990273],\n",
       "        [ 0.47804296, -0.10145801,  0.40908086]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.03080416],\n",
       "        [ 0.18761444],\n",
       "        [ 1.0743617 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are the predictions changing? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/optimizers/#available-optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers comparison in GIF ‚Üí https://mlfromscratch.com/optimizers-explained/#adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tesla's Neural Network Models is composed of 48 models trainned in 70.000 hours of GPU ‚Üí https://tesla.com/ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Year with a 8 GPU Computer ‚Üí https://twitter.com/thirdrowtesla/status/1252723358342377472"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Gradient Descent `SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `compile()` the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f81e8d0cf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_gsd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.909626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.232151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.393391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>19.506733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>14.266361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_sigmoid  pred_gsd\n",
       "abbrev                                                     \n",
       "AL       18.8             17.909626           0.0       0.0\n",
       "AK       18.1             17.232151           0.0       0.0\n",
       "AZ       18.6             16.393391           0.0       0.0\n",
       "AR       22.4             19.506733           0.0       0.0\n",
       "CA       12.0             14.266361           0.0       0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_gsd'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'pred_sgd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p1/lnbb0qwd743gqfmgynnt5w3m0000gn/T/ipykernel_36994/4078616456.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfsel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdfsel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_sgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'pred_sgd'"
     ]
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sgd)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### View History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `ADAM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `RMSPROP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it take different times to get the best accuracy? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `binary_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sparse_categorical_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mean_absolute_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mean_squared_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the end, what should be a feasible configuration of the Neural Network for this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `kernel_initializer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `activation` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `optimizer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `loss` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network's importance to find **Non-Linear Patterns** in the Data\n",
    "\n",
    "> - The number of Neurons & Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.87287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Mathematical Formula\n",
    "- Weights / Kernel Initializer\n",
    "- Loss Function\n",
    "- Activation Function\n",
    "- Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What cannot you change arbitrarily of a Neural Network?\n",
    "\n",
    "- Input Neurons\n",
    "- Output Neurons\n",
    "- Loss Functions\n",
    "- Activation Functions"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Jes√∫s L√≥pez @sotastica"
   }
  ],
  "interpreter": {
   "hash": "a2b8701b642343483c5f5e717bcb0768c6b951acf38d76a6fc8ea01492bda71d"
  },
  "kernelspec": {
   "display_name": "DeepLearning Python",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
