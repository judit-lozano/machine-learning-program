{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">#04. Why Neural Networks Deeply Learn a Mathematical Formula?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Book + Private Lessons [Here ‚Üó](https://sotastica.com/reservar)\n",
    "- Subscribe to my [Blog ‚Üó](https://blog.pythonassembly.com/)\n",
    "- Let's keep in touch on [LinkedIn ‚Üó](www.linkedin.com/in/jsulopz) üòÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning, what does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The Machine Learns...\n",
    ">\n",
    "> But, **what does it learn?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the Machine Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Practical Example ‚Üí [Tesla Autopilot](https://www.tesla.com/AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Example where It Fails ‚Üí [Tesla Confuses Moon with Semaphore](https://twitter.com/Carnage4Life/status/1418920100086784000?s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Simply execute the following lines of code to load the data.\n",
    "> - This dataset contains **statistics about Car Accidents** (columns)\n",
    "> - In each one of **USA States** (rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/fivethirtyeight/fivethirtyeight-bad-drivers-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TX</th>\n",
       "      <td>19.4</td>\n",
       "      <td>7.760</td>\n",
       "      <td>7.372</td>\n",
       "      <td>17.654</td>\n",
       "      <td>16.878</td>\n",
       "      <td>1004.75</td>\n",
       "      <td>156.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MA</th>\n",
       "      <td>8.2</td>\n",
       "      <td>1.886</td>\n",
       "      <td>2.870</td>\n",
       "      <td>7.134</td>\n",
       "      <td>6.560</td>\n",
       "      <td>1011.14</td>\n",
       "      <td>135.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CT</th>\n",
       "      <td>10.8</td>\n",
       "      <td>4.968</td>\n",
       "      <td>3.888</td>\n",
       "      <td>9.396</td>\n",
       "      <td>8.856</td>\n",
       "      <td>1068.73</td>\n",
       "      <td>167.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>4.200</td>\n",
       "      <td>3.360</td>\n",
       "      <td>10.920</td>\n",
       "      <td>10.680</td>\n",
       "      <td>878.41</td>\n",
       "      <td>165.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VA</th>\n",
       "      <td>12.7</td>\n",
       "      <td>2.413</td>\n",
       "      <td>3.429</td>\n",
       "      <td>11.049</td>\n",
       "      <td>11.176</td>\n",
       "      <td>768.95</td>\n",
       "      <td>153.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                       \n",
       "TX       19.4     7.760    7.372          17.654       16.878      1004.75   \n",
       "MA        8.2     1.886    2.870           7.134        6.560      1011.14   \n",
       "CT       10.8     4.968    3.888           9.396        8.856      1068.73   \n",
       "CA       12.0     4.200    3.360          10.920       10.680       878.41   \n",
       "VA       12.7     2.413    3.429          11.049       11.176       768.95   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "TX          156.83  \n",
       "MA          135.63  \n",
       "CT          167.02  \n",
       "CA          165.63  \n",
       "VA          153.72  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset(name='car_crashes', index_col='abbrev')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Concepts in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the `Weights`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accidents = speeding \\cdot w_1 + alcohol \\cdot w_2 \\ + ... + \\ ins\\_losses \\cdot w_7\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 12:26:09.114827: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-04 12:26:09.151113: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f81e3d51310 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-04 12:26:09.151137: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='zeros'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[ 1.0095769 ],\n",
       "        [-0.42968142],\n",
       "        [ 0.98885095]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 193.8199 - mse: 193.8199\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 867us/step - loss: 35.6187 - mse: 35.6187\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 27.7795 - mse: 27.7795\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 814us/step - loss: 27.3413 - mse: 27.3413\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 798us/step - loss: 26.8407 - mse: 26.8407\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 863us/step - loss: 26.6976 - mse: 26.6976\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 773us/step - loss: 26.5612 - mse: 26.5612\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 806us/step - loss: 26.3912 - mse: 26.3912\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 851us/step - loss: 25.7360 - mse: 25.7360\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 876us/step - loss: 25.6616 - mse: 25.6616\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 816us/step - loss: 27.3717 - mse: 27.3717\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 780us/step - loss: 30.9829 - mse: 30.9829\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 944us/step - loss: 27.4503 - mse: 27.4503\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 793us/step - loss: 24.4692 - mse: 24.4692\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 26.0772 - mse: 26.0772\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 919us/step - loss: 23.7967 - mse: 23.7967\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 836us/step - loss: 23.3207 - mse: 23.3207\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 762us/step - loss: 22.1500 - mse: 22.1500\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 821us/step - loss: 22.4584 - mse: 22.4584\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 726us/step - loss: 21.1943 - mse: 21.1943\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 733us/step - loss: 21.8399 - mse: 21.8399\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 791us/step - loss: 22.0437 - mse: 22.0437\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.6668 - mse: 19.6668\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 818us/step - loss: 20.3095 - mse: 20.3095\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 744us/step - loss: 23.4108 - mse: 23.4108\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 956us/step - loss: 18.7136 - mse: 18.7136\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 921us/step - loss: 18.0259 - mse: 18.0259\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 864us/step - loss: 20.6766 - mse: 20.6766\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 904us/step - loss: 19.3252 - mse: 19.3252\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 753us/step - loss: 28.1558 - mse: 28.1558\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 783us/step - loss: 24.9189 - mse: 24.9189\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 773us/step - loss: 18.1335 - mse: 18.1335\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 821us/step - loss: 19.8679 - mse: 19.8679\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 728us/step - loss: 16.3025 - mse: 16.3025\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 852us/step - loss: 18.9171 - mse: 18.9171\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.9006 - mse: 15.9006\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 894us/step - loss: 14.9462 - mse: 14.9462\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 898us/step - loss: 14.6052 - mse: 14.6052\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 832us/step - loss: 16.1423 - mse: 16.1423\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.6421 - mse: 16.6421\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 823us/step - loss: 13.9512 - mse: 13.9512\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 803us/step - loss: 13.6451 - mse: 13.6451\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 921us/step - loss: 18.8480 - mse: 18.8480\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 950us/step - loss: 17.2653 - mse: 17.2653\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.7577 - mse: 13.7577\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 847us/step - loss: 22.1046 - mse: 22.1046\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.4896 - mse: 13.4896\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.9146 - mse: 11.9146\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.7966 - mse: 11.7966\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.4385 - mse: 11.4385\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 996us/step - loss: 12.7734 - mse: 12.7734\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.9620 - mse: 12.9620\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.0014 - mse: 11.0014\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.8566 - mse: 10.8566\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.3559 - mse: 10.3559\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.6651 - mse: 9.6651\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 978us/step - loss: 18.6912 - mse: 18.6912\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.4150 - mse: 11.4150\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.2457 - mse: 9.2457\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.2204 - mse: 9.2204\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.0432 - mse: 9.0432\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.9210 - mse: 9.9210\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 929us/step - loss: 13.3868 - mse: 13.3868\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.2263 - mse: 13.2263\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.0550 - mse: 8.0550\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.8373 - mse: 7.8373\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4025 - mse: 8.4025\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.8908 - mse: 9.8908\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 796us/step - loss: 12.5137 - mse: 12.5137\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 927us/step - loss: 12.1366 - mse: 12.1366\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 798us/step - loss: 7.6859 - mse: 7.6859\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 905us/step - loss: 6.9544 - mse: 6.9544\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 904us/step - loss: 6.8095 - mse: 6.8095\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 824us/step - loss: 6.5395 - mse: 6.5395\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 685us/step - loss: 6.5399 - mse: 6.5399\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.3236 - mse: 6.3236\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 819us/step - loss: 15.4562 - mse: 15.4562\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 873us/step - loss: 10.0680 - mse: 10.0680\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 812us/step - loss: 7.5326 - mse: 7.5326\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 819us/step - loss: 5.9628 - mse: 5.9628\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 803us/step - loss: 7.0673 - mse: 7.0673\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 712us/step - loss: 6.4855 - mse: 6.4855\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 799us/step - loss: 5.4465 - mse: 5.4465\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.4250 - mse: 5.4250\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.7410 - mse: 5.7410\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.5867 - mse: 7.5867\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.1031 - mse: 15.1031\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.5655 - mse: 13.5655\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.8005 - mse: 5.8005\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.0051 - mse: 5.0051\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.6046 - mse: 4.6046\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 793us/step - loss: 4.9137 - mse: 4.9137\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.8857 - mse: 4.8857\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.3226 - mse: 4.3226\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.2721 - mse: 4.2721\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 767us/step - loss: 7.9103 - mse: 7.9103\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 829us/step - loss: 13.1309 - mse: 13.1309\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 992us/step - loss: 4.9635 - mse: 4.9635\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 948us/step - loss: 4.0553 - mse: 4.0553\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 960us/step - loss: 6.1442 - mse: 6.1442\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 933us/step - loss: 5.6140 - mse: 5.6140\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 918us/step - loss: 4.5237 - mse: 4.5237\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.1903 - mse: 5.1903\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.8596 - mse: 3.8596\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.6289 - mse: 3.6289\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.2977 - mse: 6.2977\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.8427 - mse: 6.8427\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.4104 - mse: 7.4104\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 754us/step - loss: 8.1031 - mse: 8.1031\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.6548 - mse: 3.6548\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 757us/step - loss: 3.3454 - mse: 3.3454\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.4603 - mse: 3.4603\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.0778 - mse: 3.0778\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 891us/step - loss: 4.6880 - mse: 4.6880\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.7705 - mse: 9.7705\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 797us/step - loss: 2.9952 - mse: 2.9952\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 798us/step - loss: 5.4051 - mse: 5.4051\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.7077 - mse: 3.7077\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.3323 - mse: 3.3323\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.1375 - mse: 6.1375\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 989us/step - loss: 4.8455 - mse: 4.8455\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.3690 - mse: 5.3690\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 747us/step - loss: 7.1405 - mse: 7.1405\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 867us/step - loss: 3.9859 - mse: 3.9859\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 811us/step - loss: 2.8818 - mse: 2.8818\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 805us/step - loss: 3.0752 - mse: 3.0752\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 769us/step - loss: 3.5110 - mse: 3.5110\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.4454 - mse: 4.4454\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 934us/step - loss: 3.1086 - mse: 3.1086\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 913us/step - loss: 3.5537 - mse: 3.5537\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 951us/step - loss: 9.0837 - mse: 9.0837\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 870us/step - loss: 9.1583 - mse: 9.1583\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.3406 - mse: 4.3406\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 935us/step - loss: 2.9769 - mse: 2.9769\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.3933 - mse: 2.3933\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 799us/step - loss: 2.4097 - mse: 2.4097\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 844us/step - loss: 2.8428 - mse: 2.8428\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.5340 - mse: 2.5340\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 739us/step - loss: 2.3221 - mse: 2.3221\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 780us/step - loss: 2.3855 - mse: 2.3855\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 830us/step - loss: 5.1695 - mse: 5.1695\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.3954 - mse: 10.3954\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.2344 - mse: 7.2344\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.2214 - mse: 2.2214\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.3902 - mse: 2.3902\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.1485 - mse: 2.1485\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.6936 - mse: 3.6936\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.5272 - mse: 8.5272\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.4468 - mse: 4.4468\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 897us/step - loss: 2.3429 - mse: 2.3429\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 738us/step - loss: 2.9084 - mse: 2.9084\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 896us/step - loss: 3.4425 - mse: 3.4425\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 946us/step - loss: 5.3540 - mse: 5.3540\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.9368 - mse: 4.9368\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.9749 - mse: 4.9749\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.1964 - mse: 2.1964\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.2237 - mse: 2.2237\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 887us/step - loss: 2.0761 - mse: 2.0761\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 788us/step - loss: 2.1418 - mse: 2.1418\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4702 - mse: 2.470 - 0s 2ms/step - loss: 2.1212 - mse: 2.1212\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.2000 - mse: 3.2000\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.5635 - mse: 5.5635\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.0536 - mse: 7.0536\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.3172 - mse: 7.3172\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 962us/step - loss: 3.5129 - mse: 3.5129\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.5394 - mse: 3.5394\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 892us/step - loss: 3.1079 - mse: 3.1079\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 833us/step - loss: 1.9634 - mse: 1.9634\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 898us/step - loss: 1.8922 - mse: 1.8922\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.9189 - mse: 1.9189\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.1386 - mse: 4.1386\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 916us/step - loss: 12.2127 - mse: 12.2127\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.0431 - mse: 3.0431\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.9081 - mse: 1.9081\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.2322 - mse: 2.2322\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 967us/step - loss: 3.2334 - mse: 3.2334\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 787us/step - loss: 3.6388 - mse: 3.6388\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 776us/step - loss: 1.8063 - mse: 1.8063\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.5426 - mse: 5.5426\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.2886 - mse: 7.2886\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.2662 - mse: 5.2662\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.7095 - mse: 4.7095\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.5227 - mse: 3.5227\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 996us/step - loss: 2.8032 - mse: 2.8032\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 954us/step - loss: 2.7458 - mse: 2.7458\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.5837 - mse: 4.5837\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.8800 - mse: 4.8800\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 780us/step - loss: 5.4144 - mse: 5.4144\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 944us/step - loss: 2.9555 - mse: 2.9555\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.7944 - mse: 1.7944\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.7793 - mse: 1.7793\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.3952 - mse: 2.3952\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.0779 - mse: 4.0779\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.2863 - mse: 8.2863\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.7344 - mse: 4.7344\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.6706 - mse: 2.6706\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 844us/step - loss: 2.0076 - mse: 2.0076\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 774us/step - loss: 1.8553 - mse: 1.8553\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 737us/step - loss: 3.1699 - mse: 3.1699\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 887us/step - loss: 6.9978 - mse: 6.9978\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 858us/step - loss: 4.6661 - mse: 4.6661\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 727us/step - loss: 3.1258 - mse: 3.1258\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 808us/step - loss: 2.3884 - mse: 2.3884\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.6819 - mse: 1.6819\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.6628 - mse: 2.6628\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 843us/step - loss: 4.9383 - mse: 4.9383\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.0385 - mse: 5.0385\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.7730 - mse: 5.7730\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.9638 - mse: 4.9638\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.2516 - mse: 2.2516\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.6319 - mse: 1.6319\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.6366 - mse: 1.6366\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 866us/step - loss: 2.1327 - mse: 2.1327\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 765us/step - loss: 2.9224 - mse: 2.9224\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 706us/step - loss: 4.7332 - mse: 4.7332\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 832us/step - loss: 4.0435 - mse: 4.0435\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 811us/step - loss: 2.7788 - mse: 2.7788\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.6807 - mse: 1.6807\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 743us/step - loss: 2.2556 - mse: 2.2556\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.3421 - mse: 7.3421\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 829us/step - loss: 4.3406 - mse: 4.3406\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 895us/step - loss: 3.1364 - mse: 3.1364\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 778us/step - loss: 5.8238 - mse: 5.8238\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 734us/step - loss: 5.1558 - mse: 5.1558\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 732us/step - loss: 3.3057 - mse: 3.3057\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 930us/step - loss: 2.2072 - mse: 2.2072\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 860us/step - loss: 1.6402 - mse: 1.6402\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 739us/step - loss: 1.7538 - mse: 1.7538\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 831us/step - loss: 2.7106 - mse: 2.7106\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 799us/step - loss: 5.4997 - mse: 5.4997\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.6509 - mse: 3.6509\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 783us/step - loss: 2.8165 - mse: 2.8165\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 735us/step - loss: 5.4414 - mse: 5.4414\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 870us/step - loss: 5.4366 - mse: 5.4366\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 698us/step - loss: 2.7899 - mse: 2.7899\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 2.0616 - mse: 2.0616\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 727us/step - loss: 1.8737 - mse: 1.8737\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 878us/step - loss: 1.7967 - mse: 1.7967\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.8302 - mse: 1.8302\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 752us/step - loss: 4.0040 - mse: 4.0040\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 883us/step - loss: 7.2193 - mse: 7.2193\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 707us/step - loss: 5.7737 - mse: 5.7737\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 747us/step - loss: 2.5016 - mse: 2.5016\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 712us/step - loss: 1.9022 - mse: 1.9022\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 743us/step - loss: 1.6935 - mse: 1.6935\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.7524 - mse: 2.7524\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 894us/step - loss: 3.3482 - mse: 3.3482\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.7420 - mse: 4.7420\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.5589 - mse: 2.5589\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 808us/step - loss: 2.2442 - mse: 2.2442\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 707us/step - loss: 1.7200 - mse: 1.7200\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 750us/step - loss: 1.6351 - mse: 1.6351\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 807us/step - loss: 1.5827 - mse: 1.5827\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 798us/step - loss: 1.5200 - mse: 1.5200\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 858us/step - loss: 4.4948 - mse: 4.4948\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 928us/step - loss: 13.2245 - mse: 13.2245\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 749us/step - loss: 3.6113 - mse: 3.6113\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.6962 - mse: 1.6962\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 757us/step - loss: 2.7265 - mse: 2.7265\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 746us/step - loss: 2.2673 - mse: 2.2673\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 729us/step - loss: 1.4817 - mse: 1.4817\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 751us/step - loss: 2.3570 - mse: 2.3570\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 899us/step - loss: 2.5511 - mse: 2.5511\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 885us/step - loss: 3.1747 - mse: 3.1747\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 843us/step - loss: 3.9366 - mse: 3.9366\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.3976 - mse: 5.3976\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 731us/step - loss: 5.4779 - mse: 5.4779\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.9092 - mse: 2.9092\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.8489 - mse: 4.8489\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 834us/step - loss: 3.9377 - mse: 3.9377\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 884us/step - loss: 1.8497 - mse: 1.8497\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.6731 - mse: 1.6731\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.2921 - mse: 2.2921\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 945us/step - loss: 6.6039 - mse: 6.6039\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 736us/step - loss: 3.3223 - mse: 3.3223\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 693us/step - loss: 1.8268 - mse: 1.8268\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.0233 - mse: 2.0233\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 869us/step - loss: 3.8920 - mse: 3.8920\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 747us/step - loss: 6.0243 - mse: 6.0243\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 960us/step - loss: 3.0391 - mse: 3.0391\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.0140 - mse: 2.0140\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 853us/step - loss: 2.5036 - mse: 2.5036\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.5703 - mse: 4.5703\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 882us/step - loss: 7.1103 - mse: 7.1103\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 852us/step - loss: 4.7956 - mse: 4.7956\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 867us/step - loss: 2.2147 - mse: 2.2147\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 904us/step - loss: 1.5761 - mse: 1.5761\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 904us/step - loss: 1.7011 - mse: 1.7011\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 997us/step - loss: 1.5648 - mse: 1.5648\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 998us/step - loss: 2.0005 - mse: 2.0005\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 877us/step - loss: 4.4682 - mse: 4.4682\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 922us/step - loss: 6.1341 - mse: 6.1341\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.2338 - mse: 4.2338\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.8120 - mse: 1.8120\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.4071 - mse: 1.4071\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 726us/step - loss: 1.9944 - mse: 1.9944\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.4728 - mse: 3.4728\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 740us/step - loss: 5.5918 - mse: 5.5918\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 717us/step - loss: 4.6854 - mse: 4.6854\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 927us/step - loss: 2.8269 - mse: 2.8269\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 931us/step - loss: 1.8815 - mse: 1.8815\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.8487 - mse: 1.8487\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 924us/step - loss: 3.9834 - mse: 3.9834\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 986us/step - loss: 7.0864 - mse: 7.0864\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 861us/step - loss: 4.8369 - mse: 4.8369\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 733us/step - loss: 1.4765 - mse: 1.4765\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 869us/step - loss: 2.3603 - mse: 2.3603\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 803us/step - loss: 1.3854 - mse: 1.3854\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.3300 - mse: 2.3300\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 828us/step - loss: 4.8253 - mse: 4.8253\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 873us/step - loss: 5.2161 - mse: 5.2161\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 914us/step - loss: 2.7976 - mse: 2.7976\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 777us/step - loss: 4.0429 - mse: 4.0429\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 763us/step - loss: 4.9245 - mse: 4.9245\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.6693 - mse: 5.6693\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 658us/step - loss: 2.8342 - mse: 2.8342\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 722us/step - loss: 1.5568 - mse: 1.5568\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 820us/step - loss: 1.4812 - mse: 1.4812\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.5051 - mse: 1.5051\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 808us/step - loss: 1.5397 - mse: 1.5397\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 984us/step - loss: 2.9779 - mse: 2.9779\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 812us/step - loss: 8.4281 - mse: 8.4281\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 713us/step - loss: 6.1256 - mse: 6.1256\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 762us/step - loss: 1.8842 - mse: 1.8842\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 825us/step - loss: 1.4105 - mse: 1.4105\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 754us/step - loss: 2.5380 - mse: 2.5380\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 728us/step - loss: 1.5977 - mse: 1.5977\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 927us/step - loss: 1.5386 - mse: 1.5386\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.9373 - mse: 2.9373\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 821us/step - loss: 2.2707 - mse: 2.2707\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 898us/step - loss: 2.5473 - mse: 2.5473\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.4387 - mse: 5.4387\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.6828 - mse: 9.6828\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.4371 - mse: 4.4371\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.2493 - mse: 2.2493\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 979us/step - loss: 1.3474 - mse: 1.3474\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.4339 - mse: 1.4339\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.3442 - mse: 1.3442\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.4280 - mse: 1.4280\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 899us/step - loss: 2.4082 - mse: 2.4082\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 911us/step - loss: 6.1045 - mse: 6.1045\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 950us/step - loss: 6.5940 - mse: 6.5940\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 768us/step - loss: 2.9433 - mse: 2.9433\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 935us/step - loss: 1.4432 - mse: 1.4432\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 955us/step - loss: 1.4467 - mse: 1.4467\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 973us/step - loss: 2.5701 - mse: 2.5701\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.4948 - mse: 4.4948\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 924us/step - loss: 5.5868 - mse: 5.5868\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.9425 - mse: 2.9425\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.3311 - mse: 1.3311\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.5815 - mse: 1.5815\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 746us/step - loss: 1.6840 - mse: 1.6840\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.4917 - mse: 3.4917\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 862us/step - loss: 10.8042 - mse: 10.8042\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 911us/step - loss: 5.5842 - mse: 5.5842\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 921us/step - loss: 2.4382 - mse: 2.4382\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 921us/step - loss: 2.6741 - mse: 2.6741\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 805us/step - loss: 1.5806 - mse: 1.5806\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 939us/step - loss: 1.7326 - mse: 1.7326\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.6267 - mse: 2.6267\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 765us/step - loss: 6.0559 - mse: 6.0559\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 962us/step - loss: 5.2808 - mse: 5.2808\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.5654 - mse: 1.5654\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 807us/step - loss: 1.3022 - mse: 1.3022\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 824us/step - loss: 1.2991 - mse: 1.2991\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 754us/step - loss: 1.6188 - mse: 1.6188\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 766us/step - loss: 3.7061 - mse: 3.7061\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 756us/step - loss: 8.4301 - mse: 8.4301\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 785us/step - loss: 4.8906 - mse: 4.8906\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 957us/step - loss: 1.8412 - mse: 1.8412\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 920us/step - loss: 1.6088 - mse: 1.6088\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 869us/step - loss: 1.6491 - mse: 1.6491\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.2798 - mse: 1.2798\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.9051 - mse: 1.9051\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.6114 - mse: 4.6114\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.6133 - mse: 4.6133\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 933us/step - loss: 3.7664 - mse: 3.7664\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 984us/step - loss: 4.4232 - mse: 4.4232\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 878us/step - loss: 2.6405 - mse: 2.6405\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 815us/step - loss: 1.3155 - mse: 1.3155\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.8582 - mse: 1.8582\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.8600 - mse: 1.8600\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 828us/step - loss: 5.8935 - mse: 5.8935\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.8331 - mse: 5.8331\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 802us/step - loss: 3.7780 - mse: 3.7780\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 825us/step - loss: 2.1502 - mse: 2.1502\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 685us/step - loss: 1.4825 - mse: 1.4825\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 762us/step - loss: 1.2777 - mse: 1.2777\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 767us/step - loss: 1.2930 - mse: 1.2930\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 692us/step - loss: 1.6437 - mse: 1.6437\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 696us/step - loss: 4.3789 - mse: 4.3789\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 733us/step - loss: 8.2513 - mse: 8.2513\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 760us/step - loss: 6.2316 - mse: 6.2316\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.8286 - mse: 1.8286\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 933us/step - loss: 1.2669 - mse: 1.2669\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3310 - mse: 1.3310\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.2583 - mse: 1.2583\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 792us/step - loss: 1.3016 - mse: 1.3016\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 840us/step - loss: 1.2244 - mse: 1.2244\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.5733 - mse: 2.5733\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.0184 - mse: 10.0184\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.7399 - mse: 3.7399\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 952us/step - loss: 1.3205 - mse: 1.3205\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.2697 - mse: 1.2697\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 887us/step - loss: 1.2693 - mse: 1.2693\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 800us/step - loss: 1.9914 - mse: 1.9914\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 943us/step - loss: 5.1768 - mse: 5.1768\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 780us/step - loss: 4.6893 - mse: 4.6893\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 995us/step - loss: 3.1935 - mse: 3.1935\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.7583 - mse: 3.7583\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.1926 - mse: 4.1926\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 899us/step - loss: 2.2107 - mse: 2.2107\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 814us/step - loss: 2.7019 - mse: 2.7019\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 740us/step - loss: 2.1964 - mse: 2.1964\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 810us/step - loss: 2.5013 - mse: 2.5013\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.1024 - mse: 4.1024\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 919us/step - loss: 3.6445 - mse: 3.6445\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 646us/step - loss: 1.7832 - mse: 1.7832\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 806us/step - loss: 1.2618 - mse: 1.2618\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 857us/step - loss: 1.3143 - mse: 1.3143\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.0229 - mse: 7.0229\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 833us/step - loss: 6.1076 - mse: 6.1076\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 862us/step - loss: 2.5346 - mse: 2.5346\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 932us/step - loss: 1.8107 - mse: 1.8107\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 877us/step - loss: 2.3805 - mse: 2.3805\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.6437 - mse: 3.6437\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.7581 - mse: 2.7581\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.4502 - mse: 1.4502\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 855us/step - loss: 1.4633 - mse: 1.4633\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 921us/step - loss: 1.1766 - mse: 1.1766\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 859us/step - loss: 2.2684 - mse: 2.2684\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 764us/step - loss: 9.0402 - mse: 9.0402\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 834us/step - loss: 6.4232 - mse: 6.4232\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 731us/step - loss: 2.0082 - mse: 2.0082\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 755us/step - loss: 1.2129 - mse: 1.2129\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 721us/step - loss: 1.2685 - mse: 1.2685\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 721us/step - loss: 1.2713 - mse: 1.2713\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.2290 - mse: 1.2290\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 858us/step - loss: 1.2207 - mse: 1.2207\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 864us/step - loss: 1.1805 - mse: 1.1805\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 962us/step - loss: 2.1075 - mse: 2.1075\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 880us/step - loss: 9.6792 - mse: 9.6792\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 783us/step - loss: 4.1272 - mse: 4.1272\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 686us/step - loss: 2.5404 - mse: 2.5404\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 813us/step - loss: 3.0225 - mse: 3.0225\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 694us/step - loss: 2.0200 - mse: 2.0200\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 729us/step - loss: 1.2986 - mse: 1.2986\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 790us/step - loss: 1.2904 - mse: 1.2904\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1921 - mse: 1.1921\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.1921 - mse: 1.1921\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 955us/step - loss: 1.2263 - mse: 1.2263\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 849us/step - loss: 3.1216 - mse: 3.1216\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 713us/step - loss: 12.4628 - mse: 12.4628\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 854us/step - loss: 4.7725 - mse: 4.7725\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 944us/step - loss: 1.2319 - mse: 1.2319\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 795us/step - loss: 1.2460 - mse: 1.2460\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 778us/step - loss: 1.6800 - mse: 1.6800\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 765us/step - loss: 2.2221 - mse: 2.2221\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 704us/step - loss: 1.5550 - mse: 1.5550\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 1.7578 - mse: 1.7578\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 746us/step - loss: 3.2840 - mse: 3.2840\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 733us/step - loss: 8.3380 - mse: 8.3380\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 825us/step - loss: 2.8152 - mse: 2.8152\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.1989 - mse: 1.1989\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 969us/step - loss: 1.7260 - mse: 1.7260\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.0624 - mse: 5.0624\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.1403 - mse: 4.1403\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.6830 - mse: 2.6830\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.9416 - mse: 2.9416\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 763us/step - loss: 5.2664 - mse: 5.2664\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.1316 - mse: 3.1316\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 790us/step - loss: 1.3343 - mse: 1.3343\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 1.3112 - mse: 1.3112\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 896us/step - loss: 2.2107 - mse: 2.2107\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 913us/step - loss: 3.6212 - mse: 3.6212\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.1969 - mse: 5.1969\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 767us/step - loss: 3.3712 - mse: 3.3712\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 766us/step - loss: 1.7415 - mse: 1.7415\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 764us/step - loss: 1.1618 - mse: 1.1618\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 741us/step - loss: 1.1793 - mse: 1.1793\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.1723 - mse: 1.1723\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 926us/step - loss: 1.6517 - mse: 1.6517\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.9330 - mse: 3.9330\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.4947 - mse: 5.4947\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 900us/step - loss: 6.0371 - mse: 6.0371\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 923us/step - loss: 2.9979 - mse: 2.9979\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 829us/step - loss: 1.2138 - mse: 1.2138\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 691us/step - loss: 1.2256 - mse: 1.2256\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 1.3608 - mse: 1.3608\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.6105 - mse: 1.6105\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.6504 - mse: 2.6504\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.7815 - mse: 7.7815\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 839us/step - loss: 2.8290 - mse: 2.8290\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 722us/step - loss: 2.3429 - mse: 2.3429\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 1.4295 - mse: 1.4295\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.4350 - mse: 1.4350\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.9725 - mse: 4.9725\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.3900 - mse: 4.3900\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 997us/step - loss: 2.4147 - mse: 2.4147\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 926us/step - loss: 3.3458 - mse: 3.3458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81e7b7da60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.07894289, -0.0787674 ,  0.07893982],\n",
       "        [ 0.18011166, -0.1799762 ,  0.18010944],\n",
       "        [ 0.12582842, -0.12567346,  0.12582575],\n",
       "        [ 0.19557348, -0.19541648,  0.1955709 ],\n",
       "        [-0.0010819 ,  0.00156696, -0.00108974],\n",
       "        [ 0.00641877, -0.00597437,  0.00641156]], dtype=float32),\n",
       " array([ 0.07594955, -0.07558998,  0.07594377], dtype=float32),\n",
       " array([[ 1.0266492 ],\n",
       "        [-0.44569474],\n",
       "        [ 1.0059054 ]], dtype=float32),\n",
       " array([0.0762404], dtype=float32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.129147]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.129147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>16.101068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.505333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>20.361097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>11.213950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit\n",
       "abbrev                             \n",
       "AL       18.8             17.129147\n",
       "AK       18.1             16.101068\n",
       "AZ       18.6             16.505333\n",
       "AR       22.4             20.361097\n",
       "CA       12.0             11.213950"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel = df[['total']].copy()\n",
    "dfsel['pred_zeros_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.413247206819417"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_zeros_after_fit)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accidents = speeding \\cdot w_1 + alcohol \\cdot w_2 \\ + ... + \\ ins\\_losses \\cdot w_7\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='ones'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-700.6162]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[ 0.11970556],\n",
       "        [-0.46308732],\n",
       "        [-0.37469077]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 613038.7500 - mse: 613038.7500\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 771us/step - loss: 580384.8750 - mse: 580384.8750\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 862us/step - loss: 558557.7500 - mse: 558557.7500\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 875us/step - loss: 540432.1875 - mse: 540432.1875\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 746us/step - loss: 524516.5625 - mse: 524516.5625\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 790us/step - loss: 509913.6562 - mse: 509913.6562\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 764us/step - loss: 496244.0000 - mse: 496244.0000\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 890us/step - loss: 483503.1875 - mse: 483503.1875\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 795us/step - loss: 471242.0312 - mse: 471242.0312\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 814us/step - loss: 459298.4688 - mse: 459298.4688\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 766us/step - loss: 447657.3438 - mse: 447657.3438\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 812us/step - loss: 436403.5625 - mse: 436403.5625\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 825us/step - loss: 425557.7188 - mse: 425557.7188\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 763us/step - loss: 414994.3125 - mse: 414994.3125\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 755us/step - loss: 404634.9062 - mse: 404634.9062\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 835us/step - loss: 394575.5938 - mse: 394575.5938\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 764us/step - loss: 384467.9062 - mse: 384467.9688\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 979us/step - loss: 374442.1250 - mse: 374442.1250\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 364877.6562 - mse: 364877.6562\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 886us/step - loss: 355472.0000 - mse: 355472.0000\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 903us/step - loss: 345897.9688 - mse: 345897.9688\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 336497.0312 - mse: 336497.0312\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 903us/step - loss: 327403.1875 - mse: 327403.1875\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 814us/step - loss: 318567.5000 - mse: 318567.5000\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 895us/step - loss: 309873.9062 - mse: 309873.9062\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 826us/step - loss: 301261.1250 - mse: 301261.1250\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 797us/step - loss: 292716.3750 - mse: 292716.3438\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 903us/step - loss: 284496.8125 - mse: 284496.8125\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 890us/step - loss: 276447.5938 - mse: 276447.5938\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 714us/step - loss: 268435.3438 - mse: 268435.3438\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 721us/step - loss: 260542.9375 - mse: 260542.9375\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 714us/step - loss: 252676.9219 - mse: 252676.9219\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 715us/step - loss: 245006.7031 - mse: 245006.7031\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 755us/step - loss: 237496.9062 - mse: 237496.9062\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 756us/step - loss: 230177.0938 - mse: 230177.0938\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 743us/step - loss: 223032.6250 - mse: 223032.6250\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 805us/step - loss: 216042.8281 - mse: 216042.8281\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 807us/step - loss: 209101.7656 - mse: 209101.7656\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 911us/step - loss: 202308.3594 - mse: 202308.3594\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 818us/step - loss: 195583.8438 - mse: 195583.8438\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 812us/step - loss: 188932.3906 - mse: 188932.3906\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 731us/step - loss: 182401.0469 - mse: 182401.0469\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 891us/step - loss: 176017.0156 - mse: 176017.0156\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 795us/step - loss: 169796.2344 - mse: 169796.2344\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 825us/step - loss: 163628.9375 - mse: 163628.9375\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 752us/step - loss: 157727.2500 - mse: 157727.2500\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 761us/step - loss: 151916.9219 - mse: 151916.9219\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 750us/step - loss: 146180.2188 - mse: 146180.2188\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 806us/step - loss: 140533.3281 - mse: 140533.3281\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 836us/step - loss: 135009.8438 - mse: 135009.8438\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 129664.8594 - mse: 129664.8594\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 124470.2656 - mse: 124470.2656\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 119348.9609 - mse: 119348.9609\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 114340.3828 - mse: 114340.3828\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 109412.0469 - mse: 109412.0469\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 104676.5859 - mse: 104676.5859\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 100079.1016 - mse: 100079.1016\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 95601.4297 - mse: 95601.4297\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 91247.7656 - mse: 91247.7656\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 87008.7812 - mse: 87008.7812\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 82882.4922 - mse: 82882.4922\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 787us/step - loss: 78798.5000 - mse: 78798.5000\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 855us/step - loss: 74796.8125 - mse: 74796.8125\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 830us/step - loss: 70947.5312 - mse: 70947.5312\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 809us/step - loss: 67245.6875 - mse: 67245.6875\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 820us/step - loss: 63688.5977 - mse: 63688.5977\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 817us/step - loss: 60196.5938 - mse: 60196.5938\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 56832.1016 - mse: 56832.1016\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 931us/step - loss: 53561.3711 - mse: 53561.3711\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 50458.5586 - mse: 50458.5586\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 47463.0547 - mse: 47463.0547\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 44528.0391 - mse: 44528.0352\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 41739.6992 - mse: 41739.6992\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 708us/step - loss: 39011.8359 - mse: 39011.8320\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 720us/step - loss: 36317.5938 - mse: 36317.5938\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 33799.5938 - mse: 33799.5938\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 31430.9258 - mse: 31430.9258\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 786us/step - loss: 29147.0059 - mse: 29147.0059\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 26974.4805 - mse: 26974.4805\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 808us/step - loss: 24887.6602 - mse: 24887.6602\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 956us/step - loss: 22906.4062 - mse: 22906.4062\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 906us/step - loss: 21004.3633 - mse: 21004.3633\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 893us/step - loss: 19170.9004 - mse: 19170.9004\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 887us/step - loss: 17461.1699 - mse: 17461.1699\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 947us/step - loss: 15833.2773 - mse: 15833.2773\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14307.2207 - mse: 14307.2207\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12854.1650 - mse: 12854.1650\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11504.4287 - mse: 11504.4287\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 839us/step - loss: 10253.0547 - mse: 10253.0547\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 737us/step - loss: 9102.5820 - mse: 9102.5820\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 733us/step - loss: 8039.0908 - mse: 8039.0908\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 718us/step - loss: 7048.8672 - mse: 7048.8672\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6131.2979 - mse: 6131.2979\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 822us/step - loss: 5308.4673 - mse: 5308.4673\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 931us/step - loss: 4564.0449 - mse: 4564.0449\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 915us/step - loss: 3879.7231 - mse: 3879.7231\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 825us/step - loss: 3254.3950 - mse: 3254.3945\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 873us/step - loss: 2702.0254 - mse: 2702.0254\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 867us/step - loss: 2223.6414 - mse: 2223.6414\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 814us/step - loss: 1807.1844 - mse: 1807.1844\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 735us/step - loss: 1445.7783 - mse: 1445.7783\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1138.1615 - mse: 1138.1615\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 985us/step - loss: 880.4493 - mse: 880.4493\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 804us/step - loss: 667.5591 - mse: 667.5591\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 794us/step - loss: 495.1653 - mse: 495.1653\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 361.9662 - mse: 361.9662\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 260.8146 - mse: 260.8146\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 878us/step - loss: 183.4515 - mse: 183.4515\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 126.6127 - mse: 126.6127\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 767us/step - loss: 87.9494 - mse: 87.9494\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 754us/step - loss: 62.3440 - mse: 62.3440\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 796us/step - loss: 45.5930 - mse: 45.5930\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 948us/step - loss: 36.2115 - mse: 36.2115\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 31.5449 - mse: 31.5449\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 777us/step - loss: 28.9609 - mse: 28.9609\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 938us/step - loss: 27.6546 - mse: 27.6546\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 953us/step - loss: 27.2294 - mse: 27.2294\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 916us/step - loss: 27.2100 - mse: 27.2100\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 27.0711 - mse: 27.0711\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 770us/step - loss: 26.9618 - mse: 26.9618\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 975us/step - loss: 26.9155 - mse: 26.9155\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 943us/step - loss: 26.8656 - mse: 26.8656\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 26.8048 - mse: 26.8048\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 881us/step - loss: 26.8480 - mse: 26.8480\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 812us/step - loss: 26.7642 - mse: 26.7642\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 800us/step - loss: 26.6945 - mse: 26.6945\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 728us/step - loss: 26.7848 - mse: 26.7848\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 755us/step - loss: 26.7526 - mse: 26.7526\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 797us/step - loss: 26.6495 - mse: 26.6495\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 824us/step - loss: 26.3324 - mse: 26.3324\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 26.2896 - mse: 26.2896\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 26.2492 - mse: 26.2492\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.4775 - mse: 28.4775\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 34.7072 - mse: 34.7072\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 32.9376 - mse: 32.9376\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 852us/step - loss: 30.5560 - mse: 30.5560\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 859us/step - loss: 41.6319 - mse: 41.6319\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 960us/step - loss: 32.3284 - mse: 32.3284\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 764us/step - loss: 25.3554 - mse: 25.3554\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 723us/step - loss: 25.4162 - mse: 25.4162\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 815us/step - loss: 25.6188 - mse: 25.6188\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 819us/step - loss: 33.1827 - mse: 33.1827\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 860us/step - loss: 38.6331 - mse: 38.6331\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 833us/step - loss: 26.7186 - mse: 26.7186\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 744us/step - loss: 26.6417 - mse: 26.6417\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 852us/step - loss: 25.3927 - mse: 25.3927\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 775us/step - loss: 45.6658 - mse: 45.6658\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 725us/step - loss: 26.1588 - mse: 26.1588\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 962us/step - loss: 24.5575 - mse: 24.5575\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 757us/step - loss: 24.0658 - mse: 24.0658\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 839us/step - loss: 26.0942 - mse: 26.0942\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 35.0444 - mse: 35.0444\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 937us/step - loss: 26.6607 - mse: 26.6607\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 975us/step - loss: 26.9945 - mse: 26.9945\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 29.5672 - mse: 29.5672\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 29.9919 - mse: 29.9919\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 23.6546 - mse: 23.6546\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.6713 - mse: 28.6713\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24.2173 - mse: 24.2173\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 906us/step - loss: 22.8144 - mse: 22.8144\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 816us/step - loss: 30.6443 - mse: 30.6443\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 775us/step - loss: 25.2551 - mse: 25.2551\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 862us/step - loss: 27.5465 - mse: 27.5465\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 970us/step - loss: 31.4214 - mse: 31.4214\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 823us/step - loss: 26.0273 - mse: 26.0273\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 930us/step - loss: 23.5265 - mse: 23.5265\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 998us/step - loss: 24.4093 - mse: 24.4093\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 825us/step - loss: 24.7135 - mse: 24.7135\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 717us/step - loss: 21.9844 - mse: 21.9844\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 761us/step - loss: 36.1019 - mse: 36.1019\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 778us/step - loss: 27.4412 - mse: 27.4412\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 828us/step - loss: 28.9880 - mse: 28.9880\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 800us/step - loss: 32.1693 - mse: 32.1693\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 815us/step - loss: 21.6679 - mse: 21.6679\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 969us/step - loss: 21.6894 - mse: 21.6894\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 706us/step - loss: 20.8983 - mse: 20.8983\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 828us/step - loss: 20.6678 - mse: 20.6678\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 860us/step - loss: 22.4830 - mse: 22.4830\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 853us/step - loss: 22.6682 - mse: 22.6682\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 22.0689 - mse: 22.0689\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 772us/step - loss: 28.4859 - mse: 28.4859\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 890us/step - loss: 46.8184 - mse: 46.8184\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 905us/step - loss: 22.4221 - mse: 22.4221\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 29.2306 - mse: 29.2306\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.0278 - mse: 23.0278\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.4685 - mse: 23.4685\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 798us/step - loss: 22.5533 - mse: 22.5533\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 807us/step - loss: 23.2379 - mse: 23.2379\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 27.7447 - mse: 27.7447\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.5801 - mse: 19.5801\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 886us/step - loss: 23.4348 - mse: 23.4348\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.0839 - mse: 19.0839\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 29.1289 - mse: 29.1289\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 39.3225 - mse: 39.3225\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.8178 - mse: 19.8178\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 777us/step - loss: 19.9301 - mse: 19.9301\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 750us/step - loss: 19.2834 - mse: 19.2834\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 711us/step - loss: 19.5986 - mse: 19.5986\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 708us/step - loss: 33.2289 - mse: 33.2289\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 908us/step - loss: 20.2929 - mse: 20.2929\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 916us/step - loss: 25.0486 - mse: 25.0486\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 22.4403 - mse: 22.4403\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 25.5000 - mse: 25.5000\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.9225 - mse: 30.9225\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 919us/step - loss: 19.8700 - mse: 19.8700\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 896us/step - loss: 21.3549 - mse: 21.3549\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 981us/step - loss: 22.4590 - mse: 22.4590\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 27.5444 - mse: 27.5444\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 988us/step - loss: 19.8916 - mse: 19.8916\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 792us/step - loss: 17.4188 - mse: 17.4188\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 762us/step - loss: 17.3236 - mse: 17.3236\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 941us/step - loss: 17.3025 - mse: 17.3025\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 839us/step - loss: 20.8792 - mse: 20.8792\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 822us/step - loss: 18.9834 - mse: 18.9834\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 889us/step - loss: 16.7886 - mse: 16.7886\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 760us/step - loss: 18.3681 - mse: 18.3681\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 757us/step - loss: 21.4543 - mse: 21.4543\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 892us/step - loss: 25.0892 - mse: 25.0892\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 795us/step - loss: 22.1739 - mse: 22.1739\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 800us/step - loss: 17.9434 - mse: 17.9434\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 875us/step - loss: 23.4551 - mse: 23.4551\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 865us/step - loss: 22.3148 - mse: 22.3148\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.4701 - mse: 17.4701\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 984us/step - loss: 16.6794 - mse: 16.6794\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 883us/step - loss: 18.7192 - mse: 18.7192\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.5822 - mse: 17.5822\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.3401 - mse: 17.3401\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.7798 - mse: 15.7798\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 17.0453 - mse: 17.0453\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 33.8646 - mse: 33.8646\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18.9609 - mse: 18.9609\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.2930 - mse: 15.2930\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.8728 - mse: 16.8728\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 997us/step - loss: 26.5119 - mse: 26.5119\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.9196 - mse: 16.9196\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.8595 - mse: 14.8595\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 968us/step - loss: 19.1472 - mse: 19.1472\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.6912 - mse: 19.6912\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 993us/step - loss: 15.1930 - mse: 15.1930\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 810us/step - loss: 14.6063 - mse: 14.6063\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 718us/step - loss: 14.3722 - mse: 14.3722\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 821us/step - loss: 14.3018 - mse: 14.3018\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 925us/step - loss: 27.1724 - mse: 27.1724\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 38.9066 - mse: 38.9066\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 811us/step - loss: 17.0237 - mse: 17.0237\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 831us/step - loss: 14.3069 - mse: 14.3069\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 916us/step - loss: 14.4638 - mse: 14.4638\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.3289 - mse: 14.3289\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18.4002 - mse: 18.4002\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.5199 - mse: 16.5199\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.7802 - mse: 28.7802\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 899us/step - loss: 21.9976 - mse: 21.9976\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.0596 - mse: 14.0596\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.4952 - mse: 14.4952\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 830us/step - loss: 14.2523 - mse: 14.2523\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 27.6158 - mse: 27.6158\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 859us/step - loss: 22.5947 - mse: 22.5947\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 990us/step - loss: 14.6386 - mse: 14.6386\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.1466 - mse: 13.1466\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.9771 - mse: 13.9771\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 738us/step - loss: 16.2601 - mse: 16.2601\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 831us/step - loss: 16.1689 - mse: 16.1689\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.0540 - mse: 14.0540\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.0109 - mse: 14.0109\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 841us/step - loss: 13.2497 - mse: 13.2497\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 830us/step - loss: 15.4241 - mse: 15.4241\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.4654 - mse: 12.4654\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.1325 - mse: 15.1325\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 32.2756 - mse: 32.2756\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 26.8489 - mse: 26.8489\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.4207 - mse: 14.4207\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.1634 - mse: 12.1634\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.2004 - mse: 14.2004\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.1402 - mse: 16.1402\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.8059 - mse: 14.8059\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.7594 - mse: 16.7594\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.9355 - mse: 17.9355\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.2874 - mse: 12.2874\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.2630 - mse: 12.2630\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.8013 - mse: 13.8013\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.2534 - mse: 14.2534\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.3824 - mse: 11.3824\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.6013 - mse: 15.6013\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 20.1233 - mse: 20.1233\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.5093 - mse: 13.5093\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.4532 - mse: 13.4532\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.8204 - mse: 12.8204\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 25.8400 - mse: 25.8400\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.1785 - mse: 23.1785\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.0304 - mse: 11.0304\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.2054 - mse: 11.2054\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.9871 - mse: 10.9871\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.3512 - mse: 11.3512\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.7058 - mse: 10.7058\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.4753 - mse: 12.4753\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 735us/step - loss: 27.5733 - mse: 27.5733\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 22.1472 - mse: 22.1472\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 785us/step - loss: 13.3583 - mse: 13.3583\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 920us/step - loss: 10.3838 - mse: 10.3838\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 996us/step - loss: 10.6828 - mse: 10.6828\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.2734 - mse: 10.2734\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 785us/step - loss: 11.6506 - mse: 11.6506\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 992us/step - loss: 11.3761 - mse: 11.3761\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 842us/step - loss: 9.9864 - mse: 9.9864\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 848us/step - loss: 14.1410 - mse: 14.1410\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 846us/step - loss: 20.2312 - mse: 20.2312\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 806us/step - loss: 17.9910 - mse: 17.9910\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.1998 - mse: 11.1998\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.0988 - mse: 10.0988\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 983us/step - loss: 11.5902 - mse: 11.5902\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 821us/step - loss: 21.8146 - mse: 21.8146\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 21.0531 - mse: 21.0531\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.6223 - mse: 10.6223\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 919us/step - loss: 12.2679 - mse: 12.2679\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.5377 - mse: 9.5377\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.5858 - mse: 9.5858\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 855us/step - loss: 18.5742 - mse: 18.5742\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.3765 - mse: 17.3765\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.8196 - mse: 12.8196\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.9424 - mse: 11.9424\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.8983 - mse: 9.8983\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.2348 - mse: 9.2348\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.4962 - mse: 10.4962\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 970us/step - loss: 18.6658 - mse: 18.6658\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 867us/step - loss: 13.6530 - mse: 13.6530\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 806us/step - loss: 9.0400 - mse: 9.0400\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 762us/step - loss: 11.0882 - mse: 11.0882\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 854us/step - loss: 11.0503 - mse: 11.0503\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 828us/step - loss: 10.7629 - mse: 10.7629\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 757us/step - loss: 21.9241 - mse: 21.9241\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.1347 - mse: 17.1347\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 883us/step - loss: 10.0687 - mse: 10.0687\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.9206 - mse: 9.9206\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 973us/step - loss: 9.9478 - mse: 9.9478\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.1436 - mse: 12.1436\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 904us/step - loss: 10.9345 - mse: 10.9345\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 722us/step - loss: 11.6567 - mse: 11.6567\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 908us/step - loss: 20.1762 - mse: 20.1762\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 766us/step - loss: 14.5336 - mse: 14.5336\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.2253 - mse: 9.2253\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 885us/step - loss: 11.3177 - mse: 11.3177\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 791us/step - loss: 13.8135 - mse: 13.8135\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 868us/step - loss: 17.8828 - mse: 17.8828\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 795us/step - loss: 13.7575 - mse: 13.7575\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 862us/step - loss: 13.4345 - mse: 13.4345\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 797us/step - loss: 14.0911 - mse: 14.0911\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 838us/step - loss: 9.1266 - mse: 9.1266\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.0105 - mse: 8.0105\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4666 - mse: 8.4666\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 814us/step - loss: 8.8247 - mse: 8.8247\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.5400 - mse: 10.5400\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 909us/step - loss: 11.9883 - mse: 11.9883\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 853us/step - loss: 13.5697 - mse: 13.5697\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 866us/step - loss: 8.5475 - mse: 8.5475\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 755us/step - loss: 8.0513 - mse: 8.0513\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 883us/step - loss: 8.4099 - mse: 8.4099\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 918us/step - loss: 25.8708 - mse: 25.8708\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 874us/step - loss: 23.6792 - mse: 23.6792\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 905us/step - loss: 9.9423 - mse: 9.9423\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 919us/step - loss: 8.4180 - mse: 8.4180\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 798us/step - loss: 10.0109 - mse: 10.0109\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 831us/step - loss: 8.4388 - mse: 8.4388\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 792us/step - loss: 7.5213 - mse: 7.5213\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 882us/step - loss: 7.5682 - mse: 7.5682\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 827us/step - loss: 7.3620 - mse: 7.3620\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 937us/step - loss: 7.6302 - mse: 7.6302\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.3759 - mse: 11.3759\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24.3483 - mse: 24.3483\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.6834 - mse: 10.6834\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.2727 - mse: 9.2727\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.9568 - mse: 12.9568\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.7531 - mse: 13.7531\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 929us/step - loss: 8.6410 - mse: 8.6410\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 879us/step - loss: 8.0791 - mse: 8.0791\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 983us/step - loss: 12.8480 - mse: 12.8480\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 880us/step - loss: 7.5765 - mse: 7.5765\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 878us/step - loss: 6.8327 - mse: 6.8327\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 826us/step - loss: 7.4252 - mse: 7.4252\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 921us/step - loss: 13.9345 - mse: 13.9345\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 20.3421 - mse: 20.3421\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 885us/step - loss: 8.3885 - mse: 8.3885\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 820us/step - loss: 6.9745 - mse: 6.9745\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 752us/step - loss: 6.9110 - mse: 6.9110\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 920us/step - loss: 6.9350 - mse: 6.9350\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 924us/step - loss: 6.7908 - mse: 6.7908\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 877us/step - loss: 6.5002 - mse: 6.5002\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 803us/step - loss: 8.1257 - mse: 8.1257\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 787us/step - loss: 21.1333 - mse: 21.1333\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 829us/step - loss: 9.9687 - mse: 9.9687\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 825us/step - loss: 8.5176 - mse: 8.5176\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 821us/step - loss: 13.4277 - mse: 13.4277\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 867us/step - loss: 10.2804 - mse: 10.2804\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.3814 - mse: 6.3814\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.3266 - mse: 6.3266\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 847us/step - loss: 11.2778 - mse: 11.2778\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 26.5451 - mse: 26.5451\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 711us/step - loss: 13.7518 - mse: 13.7518\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 904us/step - loss: 7.2907 - mse: 7.2907\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 833us/step - loss: 6.1651 - mse: 6.1651\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.6322 - mse: 8.6322\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.7242 - mse: 9.7242\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.1839 - mse: 6.1839\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.4926 - mse: 6.4926\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.1527 - mse: 7.1527\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.1651 - mse: 11.1651\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.5882 - mse: 17.5882\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 916us/step - loss: 7.5832 - mse: 7.5832\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 857us/step - loss: 5.9700 - mse: 5.9700\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.4956 - mse: 7.4956\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.3662 - mse: 11.3662\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.7818 - mse: 12.7818\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 16.5692 - mse: 16.5692\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.5087 - mse: 17.5087\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 927us/step - loss: 10.8404 - mse: 10.8404\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.0397 - mse: 6.0397\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.6261 - mse: 5.6261\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 804us/step - loss: 6.0102 - mse: 6.0102\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 985us/step - loss: 5.5992 - mse: 5.5992\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.3100 - mse: 13.3100\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 20.5534 - mse: 20.5534\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 785us/step - loss: 9.2444 - mse: 9.2444\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 876us/step - loss: 5.5485 - mse: 5.5485\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 795us/step - loss: 9.0428 - mse: 9.0428\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 833us/step - loss: 9.1246 - mse: 9.1246\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.5284 - mse: 6.5284\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.9255 - mse: 8.9255\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4668 - mse: 8.4668\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 923us/step - loss: 11.4900 - mse: 11.4900\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.3477 - mse: 10.3477\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 865us/step - loss: 10.4578 - mse: 10.4578\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 767us/step - loss: 11.0850 - mse: 11.0850\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 974us/step - loss: 11.5021 - mse: 11.5021\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.3058 - mse: 9.3058\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.0816 - mse: 7.0816\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.8407 - mse: 8.8407\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.6923 - mse: 9.6923\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 864us/step - loss: 11.6303 - mse: 11.6303\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 796us/step - loss: 6.4340 - mse: 6.4340\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 821us/step - loss: 5.6615 - mse: 5.6615\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4641 - mse: 8.4641\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 953us/step - loss: 9.0151 - mse: 9.0151\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.0038 - mse: 7.0038\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 913us/step - loss: 5.3539 - mse: 5.3539\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 700us/step - loss: 6.0384 - mse: 6.0384\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 745us/step - loss: 9.3090 - mse: 9.3090\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 932us/step - loss: 14.7590 - mse: 14.7590\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 818us/step - loss: 12.5138 - mse: 12.5138\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.5188 - mse: 9.5188\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.8303 - mse: 4.8303\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 971us/step - loss: 5.1962 - mse: 5.1962\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.6808 - mse: 6.6808\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 807us/step - loss: 11.5659 - mse: 11.5659\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 840us/step - loss: 9.0515 - mse: 9.0515\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 980us/step - loss: 7.0374 - mse: 7.0374\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 944us/step - loss: 6.7540 - mse: 6.7540\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.6828 - mse: 5.6828\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.8631 - mse: 9.8631\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 812us/step - loss: 20.3881 - mse: 20.3881\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 756us/step - loss: 8.4050 - mse: 8.4050\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 956us/step - loss: 5.6774 - mse: 5.6774\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 886us/step - loss: 5.0737 - mse: 5.0737\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 869us/step - loss: 4.5707 - mse: 4.5707\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 813us/step - loss: 9.9945 - mse: 9.9945\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 940us/step - loss: 17.5158 - mse: 17.5158\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 808us/step - loss: 9.2878 - mse: 9.2878\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 790us/step - loss: 5.1990 - mse: 5.1990\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 929us/step - loss: 4.4810 - mse: 4.4810\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 758us/step - loss: 7.5161 - mse: 7.5161\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 761us/step - loss: 11.3709 - mse: 11.3709\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 705us/step - loss: 7.7499 - mse: 7.7499\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 896us/step - loss: 4.5214 - mse: 4.5214\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.4277 - mse: 5.4277\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 818us/step - loss: 8.5661 - mse: 8.5661\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 876us/step - loss: 18.1298 - mse: 18.1298\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 847us/step - loss: 9.9908 - mse: 9.9908\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.7194 - mse: 4.7194\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 809us/step - loss: 4.2327 - mse: 4.2327\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 725us/step - loss: 6.1679 - mse: 6.1679\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 718us/step - loss: 10.8783 - mse: 10.8783\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 684us/step - loss: 8.1900 - mse: 8.1900\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 709us/step - loss: 5.7945 - mse: 5.7945\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 906us/step - loss: 5.0983 - mse: 5.0983\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 884us/step - loss: 4.6703 - mse: 4.6703\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 705us/step - loss: 9.1442 - mse: 9.1442\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 692us/step - loss: 14.4026 - mse: 14.4026\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.2294 - mse: 9.2294\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.6634 - mse: 8.6634\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.7443 - mse: 9.7443\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 767us/step - loss: 11.0850 - mse: 11.0850\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 942us/step - loss: 6.4875 - mse: 6.4875\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 784us/step - loss: 4.6883 - mse: 4.6883\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.6171 - mse: 5.6171\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4895 - mse: 8.4895\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.0018 - mse: 8.0018\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.8533 - mse: 12.8533\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4860 - mse: 8.4860\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 805us/step - loss: 4.2460 - mse: 4.2460\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 737us/step - loss: 3.9417 - mse: 3.9417\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 720us/step - loss: 3.8521 - mse: 3.8521\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.9363 - mse: 3.9363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81e8a1bdc0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.4922173 , 0.5220541 , 0.5238993 ],\n",
       "        [1.5511674 , 0.4634904 , 0.46543348],\n",
       "        [1.523666  , 0.49083027, 0.49273002],\n",
       "        [1.5268698 , 0.48760453, 0.48949757],\n",
       "        [1.1842093 , 0.8277554 , 0.8290233 ],\n",
       "        [1.2328209 , 0.7795248 , 0.7808829 ]], dtype=float32),\n",
       " array([ 0.32363573, -0.31073856, -0.3092462 ], dtype=float32),\n",
       " array([[ 0.31850177],\n",
       "        [-0.27196792],\n",
       "        [-0.18353704]], dtype=float32),\n",
       " array([0.3153119], dtype=float32)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.909641]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.909626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.232151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.393391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>19.506733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>14.266361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit\n",
       "abbrev                             \n",
       "AL       18.8             17.909626\n",
       "AK       18.1             17.232151\n",
       "AZ       18.6             16.393391\n",
       "AR       22.4             19.506733\n",
       "CA       12.0             14.266361"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel = df[['total']].copy()\n",
    "dfsel['pred_zeros_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.925355951807257"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_zeros_after_fit)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to `glorot_uniform` (default)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with the Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=558\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=558\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `sigmoid` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81e8d14310>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f81e00f65e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.909626</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.232151</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.393391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>19.506733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>14.266361</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_sigmoid\n",
       "abbrev                                           \n",
       "AL       18.8             17.909626           0.0\n",
       "AK       18.1             17.232151           0.0\n",
       "AZ       18.6             16.393391           0.0\n",
       "AR       22.4             19.506733           0.0\n",
       "CA       12.0             14.266361           0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_sigmoid'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265.98803921568634"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.7248485 ,  0.36856735,  0.63146174],\n",
       "        [ 0.07509726,  0.33351755, -0.30260724],\n",
       "        [-0.7252705 , -0.55833864,  0.15868711],\n",
       "        [-0.39576796, -0.33873704, -0.768407  ],\n",
       "        [ 0.64309967, -0.5359219 , -0.67604953],\n",
       "        [ 0.11188209, -0.31420344,  0.22522306]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.3183049 ],\n",
       "        [ 1.1042565 ],\n",
       "        [-0.02906787]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `linear` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `tanh` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `relu` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are the predictions changing? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/optimizers/#available-optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers comparison in GIF ‚Üí https://mlfromscratch.com/optimizers-explained/#adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tesla's Neural Network Models is composed of 48 models trainned in 70.000 hours of GPU ‚Üí https://tesla.com/ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Year with a 8 GPU Computer ‚Üí https://twitter.com/thirdrowtesla/status/1252723358342377472"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Gradient Descent `SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `compile()` the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f81e8d0cf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_gsd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.909626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.232151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.393391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>19.506733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>14.266361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_sigmoid  pred_gsd\n",
       "abbrev                                                     \n",
       "AL       18.8             17.909626           0.0       0.0\n",
       "AK       18.1             17.232151           0.0       0.0\n",
       "AZ       18.6             16.393391           0.0       0.0\n",
       "AR       22.4             19.506733           0.0       0.0\n",
       "CA       12.0             14.266361           0.0       0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_gsd'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'pred_sgd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p1/lnbb0qwd743gqfmgynnt5w3m0000gn/T/ipykernel_36994/4078616456.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfsel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdfsel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_sgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'pred_sgd'"
     ]
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sgd)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### View History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `ADAM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `RMSPROP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it take different times to get the best accuracy? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `binary_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sparse_categorical_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mean_absolute_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mean_squared_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the end, what should be a feasible configuration of the Neural Network for this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `kernel_initializer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `activation` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `optimizer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `loss` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network's importance to find **Non-Linear Patterns** in the Data\n",
    "\n",
    "> - The number of Neurons & Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.87287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Mathematical Formula\n",
    "- Weights / Kernel Initializer\n",
    "- Loss Function\n",
    "- Activation Function\n",
    "- Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What cannot you change arbitrarily of a Neural Network?\n",
    "\n",
    "- Input Neurons\n",
    "- Output Neurons\n",
    "- Loss Functions\n",
    "- Activation Functions"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Jes√∫s L√≥pez @sotastica"
   }
  ],
  "interpreter": {
   "hash": "a2b8701b642343483c5f5e717bcb0768c6b951acf38d76a6fc8ea01492bda71d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
