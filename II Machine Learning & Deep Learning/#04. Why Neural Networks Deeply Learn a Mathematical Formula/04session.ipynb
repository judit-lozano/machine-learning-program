{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">#04. Why Neural Networks Deeply Learn a Mathematical Formula?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Book + Private Lessons [Here ‚Üó](https://sotastica.com/reservar)\n",
    "- Subscribe to my [Blog ‚Üó](https://blog.pythonassembly.com/)\n",
    "- Let's keep in touch on [LinkedIn ‚Üó](www.linkedin.com/in/jsulopz) üòÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning, what does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The Machine Learns...\n",
    ">\n",
    "> But, **what does it learn?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the Machine Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Practical Example ‚Üí [Tesla Autopilot](https://www.tesla.com/AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Example where It Fails ‚Üí [Tesla Confuses Moon with Semaphore](https://twitter.com/Carnage4Life/status/1418920100086784000?s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Simply execute the following lines of code to load the data.\n",
    "> - This dataset contains **statistics about Car Accidents** (columns)\n",
    "> - In each one of **USA States** (rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/fivethirtyeight/fivethirtyeight-bad-drivers-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>19.5</td>\n",
       "      <td>4.095</td>\n",
       "      <td>5.655</td>\n",
       "      <td>15.990</td>\n",
       "      <td>15.795</td>\n",
       "      <td>767.91</td>\n",
       "      <td>155.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VT</th>\n",
       "      <td>13.6</td>\n",
       "      <td>4.080</td>\n",
       "      <td>4.080</td>\n",
       "      <td>13.056</td>\n",
       "      <td>12.920</td>\n",
       "      <td>716.20</td>\n",
       "      <td>109.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CO</th>\n",
       "      <td>13.6</td>\n",
       "      <td>5.032</td>\n",
       "      <td>3.808</td>\n",
       "      <td>10.744</td>\n",
       "      <td>12.920</td>\n",
       "      <td>835.50</td>\n",
       "      <td>139.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TX</th>\n",
       "      <td>19.4</td>\n",
       "      <td>7.760</td>\n",
       "      <td>7.372</td>\n",
       "      <td>17.654</td>\n",
       "      <td>16.878</td>\n",
       "      <td>1004.75</td>\n",
       "      <td>156.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>4.032</td>\n",
       "      <td>5.824</td>\n",
       "      <td>21.056</td>\n",
       "      <td>21.280</td>\n",
       "      <td>827.34</td>\n",
       "      <td>142.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                       \n",
       "TN       19.5     4.095    5.655          15.990       15.795       767.91   \n",
       "VT       13.6     4.080    4.080          13.056       12.920       716.20   \n",
       "CO       13.6     5.032    3.808          10.744       12.920       835.50   \n",
       "TX       19.4     7.760    7.372          17.654       16.878      1004.75   \n",
       "AR       22.4     4.032    5.824          21.056       21.280       827.34   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "TN          155.57  \n",
       "VT          109.61  \n",
       "CO          139.91  \n",
       "TX          156.83  \n",
       "AR          142.39  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset(name='car_crashes', index_col='abbrev')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Concepts in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the `Weights`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accidents = speeding \\cdot w_1 + alcohol \\cdot w_2 \\ + ... + \\ ins\\_losses \\cdot w_7\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='zeros'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x16a110c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 15:31:19.028496: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.1387695 ],\n",
       "        [-0.15761685],\n",
       "        [ 0.54051685]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x=AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 195.8573 - mse: 195.8573\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 34.6134 - mse: 34.6134\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 27.4192 - mse: 27.4192\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.1347 - mse: 27.1347\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 26.9614 - mse: 26.9614\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.6757 - mse: 26.6757\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.3431 - mse: 26.3431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 15:25:30.253431: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.0711 - mse: 26.0711\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 26.1092 - mse: 26.1092\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.8854 - mse: 25.8854\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.2242 - mse: 25.2242\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 25.0493 - mse: 25.0493\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.3453 - mse: 25.3453\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 24.7598 - mse: 24.7598\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 23.5612 - mse: 23.5612\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.8323 - mse: 26.8323\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.2314 - mse: 27.2314\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.9465 - mse: 26.9465\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 23.6976 - mse: 23.6976\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.7616 - mse: 22.7616\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 20.5908 - mse: 20.5908\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.5714 - mse: 21.5714\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.6800 - mse: 20.6800\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 23.0208 - mse: 23.0208\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.6328 - mse: 22.6328\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 26.9208 - mse: 26.9208\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 23.3586 - mse: 23.3586\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.1216 - mse: 18.1216\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.0194 - mse: 18.0194\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 17.5088 - mse: 17.5088\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.5940 - mse: 19.5940\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.2870 - mse: 18.2870\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.1368 - mse: 19.1368\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 15.9012 - mse: 15.9012\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 22.0648 - mse: 22.0648\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.4894 - mse: 18.4894\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.9082 - mse: 14.9082\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 16.5008 - mse: 16.5008\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.8534 - mse: 14.8534\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.1347 - mse: 16.1347\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 16.9259 - mse: 16.9259\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 13.5236 - mse: 13.5236\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.6340 - mse: 13.6340\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 13.0374 - mse: 13.0374\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.2782 - mse: 12.2782\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.8496 - mse: 12.8496\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 15.5553 - mse: 15.5553\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 16.5281 - mse: 16.5281\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 11.6185 - mse: 11.6185\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.6013 - mse: 11.6013\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 14.4047 - mse: 14.4047\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.2351 - mse: 13.2351\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.3309 - mse: 10.3309\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.3135 - mse: 10.3135\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.8531 - mse: 10.8531\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 9.6664 - mse: 9.6664\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 10.1900 - mse: 10.1900\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.7535 - mse: 14.7535\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 19.3199 - mse: 19.3199\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.7633 - mse: 12.7633\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 9.0078 - mse: 9.0078\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.3918 - mse: 11.3918\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.3647 - mse: 10.3647\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 12.1178 - mse: 12.1178\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 7.8752 - mse: 7.8752\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 7.8845 - mse: 7.8845\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.5693 - mse: 7.5693\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 7.4545 - mse: 7.4545\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.9770 - mse: 7.9770\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.5492 - mse: 7.5492\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 17.4563 - mse: 17.4563\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 8.2791 - mse: 8.2791\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 7.3985 - mse: 7.3985\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 14.4263 - mse: 14.4263\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 8.6612 - mse: 8.6612\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.6636 - mse: 6.6636\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 6.6491 - mse: 6.6491\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 7.4428 - mse: 7.4428\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.9079 - mse: 5.9079\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.2305 - mse: 7.2305\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 8.2108 - mse: 8.2108\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 6.6782 - mse: 6.6782\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.0106 - mse: 6.0106\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.9541 - mse: 6.9541\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.2938 - mse: 8.2938\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.7402 - mse: 6.7402\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.4912 - mse: 6.4912\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.4502 - mse: 5.4502\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 5.1523 - mse: 5.1523\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.5011 - mse: 5.5011\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.1464 - mse: 8.1464\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 17.0419 - mse: 17.0419\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.4384 - mse: 7.4384\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 6.2680 - mse: 6.2680\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 5.5929 - mse: 5.5929\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.2026 - mse: 4.2026\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1169 - mse: 4.1169\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.9232 - mse: 5.9232\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.6654 - mse: 7.6654\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 5.5763 - mse: 5.5763\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.8400 - mse: 3.8400\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.6966 - mse: 3.6966\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1221 - mse: 4.1221\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.5939 - mse: 3.5939\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.7211 - mse: 3.7211\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0489 - mse: 4.0489\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.8993 - mse: 11.8993\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.4790 - mse: 8.4790\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7413 - mse: 3.7413\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2584 - mse: 3.2584\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.8749 - mse: 4.8749\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.0367 - mse: 5.0367\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.5730 - mse: 3.5730\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.2251 - mse: 3.2251\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.9436 - mse: 3.9436\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0569 - mse: 4.0569\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.1306 - mse: 5.1306\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.2255 - mse: 6.2255\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.8963 - mse: 5.8963\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 5.4556 - mse: 5.4556\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.2779 - mse: 6.2779\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 8.4870 - mse: 8.4870\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.7843 - mse: 2.7843\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.7399 - mse: 3.7399\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 4.6048 - mse: 4.6048\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.4465 - mse: 3.4465\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.8105 - mse: 2.8105\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8314 - mse: 2.8314\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0620 - mse: 3.0620\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.6292 - mse: 2.6292\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.8014 - mse: 5.8014\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.8040 - mse: 11.8040\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.4253 - mse: 4.4253\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.4618 - mse: 2.4618\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4383 - mse: 2.4383\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.7100 - mse: 3.7100\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.1981 - mse: 3.1981\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.7082 - mse: 6.7082\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.0986 - mse: 8.0986\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.9361 - mse: 3.9361\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4491 - mse: 2.4491\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3094 - mse: 2.3094\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1986 - mse: 2.1986\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.8435 - mse: 6.8435\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.1338 - mse: 7.1338\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9175 - mse: 2.9175\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8958 - mse: 2.8958\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1362 - mse: 2.1362\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2302 - mse: 3.2302\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.0379 - mse: 7.0379\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.6570 - mse: 5.6570\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.6084 - mse: 3.6084\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0158 - mse: 4.0158\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 5.6392 - mse: 5.6392\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.0511 - mse: 3.0511\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.7698 - mse: 2.7698\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.0166 - mse: 5.0166\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.7299 - mse: 5.7299\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.1406 - mse: 4.1406\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9080 - mse: 2.9080\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 6.3285 - mse: 6.3285\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 6.1796 - mse: 6.1796\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.4165 - mse: 3.4165\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8271 - mse: 2.8271\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.3005 - mse: 3.3005\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 6.0122 - mse: 6.0122\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.2372 - mse: 5.2372\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.5569 - mse: 3.5569\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7538 - mse: 2.7538\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9301 - mse: 1.9301\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7109 - mse: 2.7109\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 5.7551 - mse: 5.7551\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 9.4455 - mse: 9.4455\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.2884 - mse: 4.2884\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1548 - mse: 2.1548\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.9011 - mse: 1.9011\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.0712 - mse: 2.0712\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9289 - mse: 2.9289\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.9996 - mse: 6.9996\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.2859 - mse: 5.2859\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9541 - mse: 2.9541\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3007 - mse: 2.3007\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.2255 - mse: 2.2255\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.9665 - mse: 4.9665\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.2352 - mse: 4.2352\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2200 - mse: 3.2200\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.6721 - mse: 2.6721\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.9913 - mse: 4.9913\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.0466 - mse: 6.0466\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0137 - mse: 3.0137\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4809 - mse: 2.4809\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1329 - mse: 4.1329\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6540 - mse: 2.6540\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.2930 - mse: 2.2930\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.5182 - mse: 3.5182\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 4.9134 - mse: 4.9134\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.2264 - mse: 4.2264\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.3512 - mse: 4.3512\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.0254 - mse: 6.0254\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.8923 - mse: 3.8923\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6109 - mse: 2.6109\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4740 - mse: 2.4740\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.8747 - mse: 2.8747\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.9431 - mse: 4.9431\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.3217 - mse: 4.3217\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.8626 - mse: 3.8626\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0126 - mse: 4.0126\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3430 - mse: 2.3430\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4668 - mse: 2.4668\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7883 - mse: 1.7883\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0426 - mse: 2.0426\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.8980 - mse: 3.8980\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2506 - mse: 3.2506\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1173 - mse: 4.1173\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.0789 - mse: 5.0789\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.6362 - mse: 3.6362\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7406 - mse: 2.7406\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9646 - mse: 1.9646\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8128 - mse: 1.8128\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.1749 - mse: 2.1749\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.0707 - mse: 5.0707\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.0001 - mse: 8.0001\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.7664 - mse: 3.7664\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.2186 - mse: 2.2186\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5797 - mse: 2.5797\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.3023 - mse: 3.3023\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 5.6273 - mse: 5.6273\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 5.0392 - mse: 5.0392\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.2853 - mse: 2.2853\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.3717 - mse: 2.3717\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.4505 - mse: 4.4505\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.6492 - mse: 4.6492\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.1148 - mse: 4.1148\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.5881 - mse: 3.5881\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.2153 - mse: 4.2153\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3480 - mse: 2.3480\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.0010 - mse: 3.0010\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.5484 - mse: 3.5484\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1776 - mse: 2.1776\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1585 - mse: 2.1585\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.7716 - mse: 1.7716\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5997 - mse: 1.5997\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4708 - mse: 2.4708\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.3955 - mse: 6.3955\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.3225 - mse: 9.3225\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.8239 - mse: 2.8239\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.8965 - mse: 1.8965\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.8844 - mse: 2.8844\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.5783 - mse: 3.5783\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0994 - mse: 4.0994\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 6.5690 - mse: 6.5690\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.1832 - mse: 3.1832\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.2208 - mse: 2.2208\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.7707 - mse: 1.7707\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5536 - mse: 1.5536\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.5388 - mse: 1.5388\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.4480 - mse: 3.4480\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 8.8030 - mse: 8.8030\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4854 - mse: 2.4854\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.5480 - mse: 1.5480\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5558 - mse: 1.5558\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.7393 - mse: 1.7393\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3814 - mse: 2.3814\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.0806 - mse: 8.0806\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.1048 - mse: 9.1048\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.0667 - mse: 3.0667\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7891 - mse: 1.7891\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7392 - mse: 1.7392\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6048 - mse: 2.6048\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0673 - mse: 4.0673\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8352 - mse: 2.8352\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4198 - mse: 2.4198\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.5699 - mse: 4.5699\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.5126 - mse: 3.5126\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9567 - mse: 1.9567\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4577 - mse: 2.4577\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.1980 - mse: 6.1980\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.9268 - mse: 4.9268\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.9308 - mse: 3.9308\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.5831 - mse: 3.5831\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.7672 - mse: 2.7672\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6388 - mse: 1.6388\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7932 - mse: 1.7932\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.9762 - mse: 2.9762\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.4733 - mse: 7.4733\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.6380 - mse: 4.6380\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.5311 - mse: 1.5311\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.9385 - mse: 2.9385\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7807 - mse: 1.7807\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8708 - mse: 1.8708\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.6998 - mse: 2.6998\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.0341 - mse: 6.0341\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1569 - mse: 4.1569\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.7415 - mse: 1.7415\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.7859 - mse: 1.7859\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0690 - mse: 2.0690\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9036 - mse: 2.9036\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.6226 - mse: 5.6226\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.0373 - mse: 9.0373\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.8419 - mse: 4.8419\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.5421 - mse: 3.5421\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9958 - mse: 2.9958\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6334 - mse: 1.6334\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4210 - mse: 1.4210\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3451 - mse: 2.3451\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.1441 - mse: 3.1441\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8070 - mse: 1.8070\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3947 - mse: 1.3947\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.7831 - mse: 1.7831\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.8871 - mse: 3.8871\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.4772 - mse: 9.4772\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.8985 - mse: 4.8985\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.1492 - mse: 3.1492\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9346 - mse: 1.9346\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.4932 - mse: 1.4932\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.3911 - mse: 1.3911\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.4144 - mse: 1.4144\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.3827 - mse: 1.3827\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.6197 - mse: 1.6197\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.3209 - mse: 4.3209\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.1844 - mse: 11.1844\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.8131 - mse: 4.8131\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.5652 - mse: 1.5652\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4999 - mse: 1.4999\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9470 - mse: 1.9470\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.3941 - mse: 1.3941\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3831 - mse: 1.3831\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0886 - mse: 2.0886\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.6384 - mse: 8.6384\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2973 - mse: 4.2973\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6594 - mse: 1.6594\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8839 - mse: 1.8839\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.6242 - mse: 2.6242\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.2884 - mse: 3.2884\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 4.0654 - mse: 4.0654\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.4370 - mse: 4.4370\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.8147 - mse: 2.8147\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.9817 - mse: 1.9817\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.2074 - mse: 6.2074\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.9367 - mse: 6.9367\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.1552 - mse: 5.1552\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.0210 - mse: 3.0210\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4898 - mse: 1.4898\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.5022 - mse: 1.5022\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6676 - mse: 1.6676\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9784 - mse: 2.9784\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.5204 - mse: 5.5204\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.2608 - mse: 7.2608\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.6407 - mse: 3.6407\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2298 - mse: 3.2298\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.1679 - mse: 3.1679\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3939 - mse: 2.3939\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5630 - mse: 1.5630\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3571 - mse: 1.3571\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.5014 - mse: 1.5014\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1691 - mse: 4.1691\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.9491 - mse: 6.9491\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.1425 - mse: 2.1425\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4080 - mse: 1.4080\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8687 - mse: 1.8687\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.2212 - mse: 4.2212\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.5479 - mse: 6.5479\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 4.4411 - mse: 4.4411\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.5856 - mse: 2.5856\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.5772 - mse: 1.5772\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.6932 - mse: 1.6932\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.7397 - mse: 2.7397\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5669 - mse: 2.5669\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7782 - mse: 2.7782\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.4551 - mse: 4.4551\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.0676 - mse: 3.0676\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.2374 - mse: 3.2374\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.6244 - mse: 4.6244\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.1593 - mse: 2.1593\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.6610 - mse: 1.6610\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3361 - mse: 2.3361\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.2320 - mse: 3.2320\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.1179 - mse: 7.1179\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.1954 - mse: 3.1954\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3029 - mse: 1.3029\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5400 - mse: 1.5400\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7517 - mse: 1.7517\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7173 - mse: 1.7173\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4021 - mse: 2.4021\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.1593 - mse: 9.1593\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.5540 - mse: 3.5540\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4717 - mse: 1.4717\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7142 - mse: 1.714 - 0s 5ms/step - loss: 1.2796 - mse: 1.2796\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2700 - mse: 1.2700\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7309 - mse: 1.7309\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.4371 - mse: 6.4371\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.9071 - mse: 5.9071\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6803 - mse: 1.6803\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3370 - mse: 1.3370\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3044 - mse: 1.3044\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.3842 - mse: 1.3842\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.3700 - mse: 4.3700\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 7.7982 - mse: 7.7982\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.0273 - mse: 5.0273\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.6847 - mse: 3.6847\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8370 - mse: 1.8370\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4924 - mse: 1.4924\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.2548 - mse: 1.2548\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2458 - mse: 1.2458\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.1697 - mse: 2.1697\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.8516 - mse: 7.8516\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.6714 - mse: 6.6714\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.3212 - mse: 3.3212\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6766 - mse: 1.6766\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5102 - mse: 1.5102\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.3033 - mse: 1.3033\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3329 - mse: 1.3329\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4440 - mse: 1.4440\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.4818 - mse: 3.4818\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.2259 - mse: 7.2259\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.8422 - mse: 4.8422\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.9559 - mse: 1.9559\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.4187 - mse: 1.4187\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.2467 - mse: 1.2467\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.4314 - mse: 1.4314\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2332 - mse: 1.2332\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.2692 - mse: 2.2692\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.7562 - mse: 5.7562\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.0146 - mse: 7.0146\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.1769 - mse: 4.1769\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3288 - mse: 1.3288\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.2273 - mse: 1.2273\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4691 - mse: 1.4691\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3402 - mse: 2.3402\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9978 - mse: 2.9978\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9940 - mse: 2.9940\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.9099 - mse: 2.9099\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1403 - mse: 3.1403\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.0383 - mse: 6.0383\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.7780 - mse: 4.7780\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.0331 - mse: 2.0331\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0135 - mse: 2.0135\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7125 - mse: 2.7125\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9471 - mse: 2.9471\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4455 - mse: 1.4455\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2124 - mse: 1.2124\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.1998 - mse: 1.1998\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0079 - mse: 2.0079\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.4972 - mse: 7.4972\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.2152 - mse: 5.2152\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9037 - mse: 1.9037\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2283 - mse: 1.2283\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7612 - mse: 1.7612\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.3511 - mse: 3.3511\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.9692 - mse: 3.9692\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.6938 - mse: 3.6938\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5051 - mse: 2.5051\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.9387 - mse: 3.9387\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.7005 - mse: 4.7005\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1723 - mse: 3.1723\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4592 - mse: 1.4592\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6000 - mse: 1.6000\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5570 - mse: 1.5570\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7081 - mse: 1.7081\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.6787 - mse: 3.6787\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.6055 - mse: 6.6055\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9113 - mse: 2.9113\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4515 - mse: 1.4515\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2878 - mse: 1.2878\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.1876 - mse: 3.1876\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.1095 - mse: 8.1095\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.4296 - mse: 3.4296\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8701 - mse: 1.8701\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.2282 - mse: 2.2282\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7176 - mse: 2.7176\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2966 - mse: 2.2966\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3967 - mse: 2.3967\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8939 - mse: 1.8939\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2736 - mse: 1.2736\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.7887 - mse: 1.7887\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.1399 - mse: 7.1399\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.7776 - mse: 7.7776\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8723 - mse: 2.8723\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6306 - mse: 1.6306\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3440 - mse: 2.3440\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8021 - mse: 2.8021\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.6296 - mse: 1.6296\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2080 - mse: 1.2080\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2175 - mse: 1.2175\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.2652 - mse: 1.2652\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0642 - mse: 2.0642\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.4098 - mse: 10.4098\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.9830 - mse: 5.9830\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4667 - mse: 1.4667\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6561 - mse: 1.6561\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.1961 - mse: 1.1961\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3375 - mse: 1.3375\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.3608 - mse: 1.3608\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5915 - mse: 1.5915\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7728 - mse: 2.7728\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.3226 - mse: 7.3226\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2209 - mse: 3.2209\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4018 - mse: 2.4018\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1043 - mse: 4.1043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1686286d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.08066401, -0.08103037,  0.08105689],\n",
       "        [ 0.17918132, -0.17946328,  0.17948398],\n",
       "        [ 0.12544397, -0.1257719 ,  0.12579569],\n",
       "        [ 0.19439285, -0.19473135,  0.19475527],\n",
       "        [-0.00223334,  0.00116586, -0.00109652],\n",
       "        [ 0.00606154, -0.00703107,  0.0070945 ]], dtype=float32),\n",
       " array([ 0.07513256, -0.07591349,  0.07596511], dtype=float32),\n",
       " array([[ 0.26604873],\n",
       "        [-1.010353  ],\n",
       "        [ 1.220803  ]], dtype=float32),\n",
       " array([0.07621886], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 15:26:07.527944: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[17.336948]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.336948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>16.232738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.624100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>20.550613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>11.388576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit\n",
       "abbrev                             \n",
       "AL       18.8             17.336948\n",
       "AK       18.1             16.232738\n",
       "AZ       18.6             16.624100\n",
       "AR       22.4             20.550613\n",
       "CA       12.0             11.388576"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel = df[['total']].copy()\n",
    "dfsel['pred_zeros_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9455144617939495"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_zeros_after_fit)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accidents = speeding \\cdot w_1 + alcohol \\cdot w_2 \\ + ... + \\ ins\\_losses \\cdot w_7\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='ones'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 15:26:44.322688: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-774.39435]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.88074064],\n",
       "        [ 1.1793224 ],\n",
       "        [-1.0922706 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 743380.5625 - mse: 743380.5625\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 688056.8125 - mse: 688056.8125\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 651155.6875 - mse: 651155.6875\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 621083.8750 - mse: 621083.8750\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 594540.4375 - mse: 594540.4375\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 570920.9375 - mse: 570920.9375\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 549101.5625 - mse: 549101.5625\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 528504.3125 - mse: 528504.3125\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 508640.0938 - mse: 508640.0938\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 489527.0312 - mse: 489527.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 15:27:17.017965: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 471250.4375 - mse: 471250.4375\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 453444.1875 - mse: 453444.1875\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 436369.6875 - mse: 436369.6875\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 419936.0938 - mse: 419936.0938\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 403797.1562 - mse: 403797.1562\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 388223.9375 - mse: 388223.9375\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 373107.3125 - mse: 373107.3125\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 358353.3438 - mse: 358353.3438\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 343992.6250 - mse: 343992.6250\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 330173.1875 - mse: 330173.1875\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 316842.8750 - mse: 316842.8750\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 303798.9062 - mse: 303798.9062\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 290866.5938 - mse: 290866.5938\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 278293.6250 - mse: 278293.6250\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 266062.5938 - mse: 266062.5938\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 253964.2500 - mse: 253964.2500\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 242116.8750 - mse: 242116.8750\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 230667.8906 - mse: 230667.8906\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 219775.8125 - mse: 219775.8125\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 209207.5312 - mse: 209207.5312\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 198722.5000 - mse: 198722.5000\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 188730.5000 - mse: 188730.5000\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 179000.0469 - mse: 179000.0469\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 169253.2344 - mse: 169253.2344\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 160007.5469 - mse: 160007.5469\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 151165.0312 - mse: 151165.0312\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 142503.0000 - mse: 142503.0000\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 134155.2812 - mse: 134155.2812\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 126025.0859 - mse: 126025.0859\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 117999.2109 - mse: 117999.2109\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 110444.7812 - mse: 110444.7812\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 103250.6094 - mse: 103250.6094\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 96363.1641 - mse: 96363.1641\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 89747.0156 - mse: 89747.0156\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 83453.3203 - mse: 83453.3203\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 77355.0781 - mse: 77355.0781\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 71406.4609 - mse: 71406.4609\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 65849.9453 - mse: 65849.9453\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 60577.7266 - mse: 60577.7266\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 55530.5117 - mse: 55530.5117\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 50735.6289 - mse: 50735.6289\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 46228.4375 - mse: 46228.4375\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 41999.0312 - mse: 41999.0273\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 37939.9336 - mse: 37939.9336\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 34155.5625 - mse: 34155.5625\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 30610.0117 - mse: 30610.0117\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27273.2500 - mse: 27273.2500\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 24183.4863 - mse: 24183.4863\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21348.5742 - mse: 21348.5742\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18711.3730 - mse: 18711.3730\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 16311.3682 - mse: 16311.3682\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 14113.2881 - mse: 14113.2881\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12085.7148 - mse: 12085.7148\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 10245.0615 - mse: 10245.0605\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 8607.5332 - mse: 8607.5332\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 7152.8564 - mse: 7152.8564\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5857.2300 - mse: 5857.2300\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4732.8848 - mse: 4732.8848\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3760.2354 - mse: 3760.2354\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2932.6355 - mse: 2932.6355\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2243.7747 - mse: 2243.7747\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1685.1560 - mse: 1685.1560\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1238.6256 - mse: 1238.6256\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 883.3278 - mse: 883.3278\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 613.6887 - mse: 613.6887\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 414.9252 - mse: 414.9252\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 274.2207 - mse: 274.2207\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 177.2920 - mse: 177.2920\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 113.0628 - mse: 113.0628\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 72.9816 - mse: 72.9816\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 49.4503 - mse: 49.4503\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 36.6451 - mse: 36.6451\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 30.9252 - mse: 30.9252\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 28.4945 - mse: 28.4945\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27.2559 - mse: 27.2559\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.7515 - mse: 26.7515\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.4952 - mse: 26.4952\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.2917 - mse: 26.2917\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.3399 - mse: 26.3399\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 26.2310 - mse: 26.2310\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.1874 - mse: 26.1874\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.8921 - mse: 25.8921\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 26.0777 - mse: 26.0777\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.0025 - mse: 26.0025\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.2772 - mse: 26.2772\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.0247 - mse: 26.0247\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 25.8865 - mse: 25.8865\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27.1440 - mse: 27.1440\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 25.1512 - mse: 25.1512\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 24.8829 - mse: 24.8829\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 24.9304 - mse: 24.9304\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 24.4314 - mse: 24.4314\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 24.9068 - mse: 24.9068\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 23.2129 - mse: 23.2129\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 23.6941 - mse: 23.6941\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 22.6350 - mse: 22.6350\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 22.5158 - mse: 22.5158\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 29.1163 - mse: 29.1163\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 54.0740 - mse: 54.0740\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 49.5758 - mse: 49.5758\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.5670 - mse: 27.5670\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 28.3257 - mse: 28.3257\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 28.0766 - mse: 28.0766\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 23.4493 - mse: 23.4493\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 36.3249 - mse: 36.3249\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 37.6294 - mse: 37.6294\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 46.8596 - mse: 46.8596\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 26.1468 - mse: 26.1468\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.1845 - mse: 20.1845\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.6393 - mse: 20.6393\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 20.2493 - mse: 20.2493\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.5717 - mse: 27.5717\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 56.0819 - mse: 56.0819\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.1817 - mse: 27.1817\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 17.2007 - mse: 17.2007\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 18.1789 - mse: 18.1789\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 40.0697 - mse: 40.0697\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.7279 - mse: 20.7279\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.1466 - mse: 16.1466\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 16.9494 - mse: 16.9494\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 33.0973 - mse: 33.0973\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 52.9479 - mse: 52.9479\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 34.6962 - mse: 34.6962\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.4383 - mse: 20.4383\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.0704 - mse: 18.0704\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.6078 - mse: 22.6078\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 38.7803 - mse: 38.7803\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 24.4544 - mse: 24.4544\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 21.5859 - mse: 21.5859\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 35.0883 - mse: 35.0883\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 34.6114 - mse: 34.6114\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 22.3869 - mse: 22.3869\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 19.4739 - mse: 19.4739\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 19.4623 - mse: 19.4623\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 21.6591 - mse: 21.6591\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.3066 - mse: 14.3066\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.0602 - mse: 12.0602\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 17.2319 - mse: 17.2319\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 25.0815 - mse: 25.0815\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.4138 - mse: 22.4138\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 30.9617 - mse: 30.9617\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 42.3962 - mse: 42.3962\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 24.5722 - mse: 24.5722\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.6526 - mse: 12.6526\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.8493 - mse: 11.8493\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.4359 - mse: 18.4359\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 16.3972 - mse: 16.3972\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 14.0548 - mse: 14.0548\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 18.5643 - mse: 18.5643\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.3252 - mse: 21.3252\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.7897 - mse: 26.7897\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.1349 - mse: 22.1349\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 26.4644 - mse: 26.4644\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.1888 - mse: 12.1888\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 18.7054 - mse: 18.7054\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.3551 - mse: 13.3551\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.6456 - mse: 9.6456\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.7201 - mse: 8.7201\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.5359 - mse: 8.5359\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 12.6907 - mse: 12.6907\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 42.0924 - mse: 42.0924\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 31.3778 - mse: 31.3778\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 12.7076 - mse: 12.7076\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.5658 - mse: 10.5658\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 21.5364 - mse: 21.5364\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 37.2573 - mse: 37.2573\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 19.9532 - mse: 19.9532\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.6397 - mse: 14.6397\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.7898 - mse: 19.7898\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 11.0813 - mse: 11.0813\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.9453 - mse: 10.9453\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.8632 - mse: 7.8632\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.2351 - mse: 7.2351\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.5415 - mse: 9.5415\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 23.5050 - mse: 23.5050\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 30.2772 - mse: 30.2772\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 23.9744 - mse: 23.9744\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.8546 - mse: 17.8546\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.9048 - mse: 12.9048\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.3654 - mse: 9.3654\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.2203 - mse: 7.2203\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.2280 - mse: 6.2280\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.3427 - mse: 6.3427\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.0539 - mse: 6.0539\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.2659 - mse: 7.2659\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 50.5694 - mse: 50.5694\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 35.4845 - mse: 35.4845\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 9.4345 - mse: 9.4345\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.1044 - mse: 6.1044\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.8735 - mse: 5.8735\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 5.7900 - mse: 5.7900\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.6373 - mse: 7.6373\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.8531 - mse: 26.8531\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 34.6290 - mse: 34.6290\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 10.4886 - mse: 10.4886\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.4450 - mse: 5.4450\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.5696 - mse: 5.5696\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.9680 - mse: 9.9680\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 35.2203 - mse: 35.2203\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 28.2278 - mse: 28.2278\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 12.0324 - mse: 12.0324\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.4245 - mse: 12.4245\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.8726 - mse: 9.8726\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.6040 - mse: 9.6040\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 11.2282 - mse: 11.2282\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.9233 - mse: 4.9233\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.1376 - mse: 6.1376\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 19.2418 - mse: 19.2418\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 40.4576 - mse: 40.4576\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.7610 - mse: 25.7610\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.7980 - mse: 12.7980\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.8291 - mse: 5.8291\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.2820 - mse: 4.2820\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2364 - mse: 4.2364\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.8172 - mse: 4.8172\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.3492 - mse: 11.3492\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 35.7604 - mse: 35.7604\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 29.5749 - mse: 29.5749\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.8166 - mse: 14.8166\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.4539 - mse: 9.4539\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.7459 - mse: 7.7459\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.4093 - mse: 8.4093\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.3793 - mse: 5.3793\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.2188 - mse: 7.2188\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 20.4188 - mse: 20.4188\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 40.9005 - mse: 40.9005\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 23.0019 - mse: 23.0019\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.6896 - mse: 12.6896\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.0115 - mse: 6.0115\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.4759 - mse: 5.4759\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.9392 - mse: 5.9392\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.7194 - mse: 6.7194\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.9046 - mse: 7.9046\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.8203 - mse: 12.8203\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.1986 - mse: 16.1986\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 32.2234 - mse: 32.2234\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.4582 - mse: 16.4582\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.8408 - mse: 13.8408\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.5611 - mse: 6.5611\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.3859 - mse: 4.3859\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.2174 - mse: 4.2174\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.1185 - mse: 7.1185\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.6710 - mse: 7.6710\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 16.7438 - mse: 16.7438\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 38.4476 - mse: 38.4476\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.3271 - mse: 21.3271\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.9813 - mse: 13.9813\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.5429 - mse: 4.5429\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.3558 - mse: 3.3558\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.0520 - mse: 3.0520\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.3859 - mse: 3.3859\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2983 - mse: 3.2983\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.8404 - mse: 3.8404\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.1427 - mse: 11.1427\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 58.0111 - mse: 58.0111\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.5883 - mse: 13.5883\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.5985 - mse: 7.5985\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.6005 - mse: 6.6005\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.5422 - mse: 6.5422\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.5566 - mse: 10.5566\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.8650 - mse: 15.8650\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.6313 - mse: 15.6313\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 12.6688 - mse: 12.6688\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.2412 - mse: 12.2412\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.0587 - mse: 13.0587\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.3720 - mse: 16.3720\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.4624 - mse: 20.4624\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.9337 - mse: 14.9337\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.8967 - mse: 8.8967\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.0802 - mse: 11.0802\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 16.9574 - mse: 16.9574\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 11.8756 - mse: 11.8756\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.7924 - mse: 7.7924\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.5771 - mse: 5.5771\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.7159 - mse: 11.7159\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.3115 - mse: 22.3115\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 15.2592 - mse: 15.2592\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.8441 - mse: 5.8441\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4825 - mse: 2.4825\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.6566 - mse: 2.6566\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.9091 - mse: 3.9091\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 9.3387 - mse: 9.3387\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 37.8454 - mse: 37.8454\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 30.8974 - mse: 30.8974\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 10.9178 - mse: 10.9178\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9730 - mse: 2.9730\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.5424 - mse: 3.5424\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.9066 - mse: 8.9066\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 23.6372 - mse: 23.6372\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 30.0031 - mse: 30.0031\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.2811 - mse: 8.2811\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 6.1500 - mse: 6.1500\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.8467 - mse: 6.8467\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 12.8051 - mse: 12.8051\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 11.3458 - mse: 11.3458\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.5267 - mse: 12.5267\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 16.9647 - mse: 16.9647\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.8821 - mse: 7.8821\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.7770 - mse: 4.7770\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.5752 - mse: 6.5752\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 19.8197 - mse: 19.8197\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.5609 - mse: 22.5609\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 15.7611 - mse: 15.7611\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.7074 - mse: 2.7074\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.3566 - mse: 3.3566\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 12.5895 - mse: 12.5895\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 21.4195 - mse: 21.4195\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.9449 - mse: 8.9449\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.4569 - mse: 9.4569\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.5646 - mse: 10.5646\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 15.0360 - mse: 15.0360\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.4817 - mse: 20.4817\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.1747 - mse: 18.1747\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.2328 - mse: 14.2328\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.3119 - mse: 9.3119\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 6.2201 - mse: 6.2201\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 13.3687 - mse: 13.3687\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.7112 - mse: 9.7112\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.2001 - mse: 12.2001\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 12.1414 - mse: 12.1414\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 7.8820 - mse: 7.8820\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.9323 - mse: 10.9323\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 17.9581 - mse: 17.9581\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.8824 - mse: 21.8824\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 19.9561 - mse: 19.9561\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 9.9248 - mse: 9.9248\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.4986 - mse: 6.4986\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.0426 - mse: 6.0426\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.6799 - mse: 4.6799\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.9408 - mse: 7.9408\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.4624 - mse: 20.4624\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 24.7300 - mse: 24.7300\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.4334 - mse: 10.4334\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 7.7947 - mse: 7.794 - 0s 5ms/step - loss: 7.3805 - mse: 7.3805\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.6378 - mse: 5.6378\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8845 - mse: 2.8845\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.9353 - mse: 3.9353\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.9623 - mse: 8.9623\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.9921 - mse: 26.9921\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 28.6640 - mse: 28.6640\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.1559 - mse: 8.1559\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.5055 - mse: 4.5055\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.3222 - mse: 7.3222\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.0868 - mse: 9.0868\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.8644 - mse: 6.8644\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.1413 - mse: 11.1413\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.4425 - mse: 26.4425\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.1691 - mse: 19.1691\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.8658 - mse: 17.8658\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.2224 - mse: 7.2224\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.1544 - mse: 5.1544\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.2002 - mse: 10.2002\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.7059 - mse: 11.7059\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 9.3931 - mse: 9.3931\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.3335 - mse: 15.3335\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 14.4316 - mse: 14.4316\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 13.0089 - mse: 13.0089\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 14.0798 - mse: 14.0798\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.9075 - mse: 6.9075\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.5709 - mse: 8.5709\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.9038 - mse: 4.9038\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7867 - mse: 3.7867\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.3029 - mse: 11.3029\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 26.7528 - mse: 26.7528\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 21.0174 - mse: 21.0174\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.6253 - mse: 6.6253\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.0047 - mse: 4.0047\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 5.8065 - mse: 5.8065\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.9240 - mse: 8.9240\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.7428 - mse: 25.7428\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 24.5878 - mse: 24.5878\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.7709 - mse: 6.7709\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.5767 - mse: 4.5767\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.4792 - mse: 4.4792\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.9321 - mse: 5.9321\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.8376 - mse: 11.8376\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.4741 - mse: 22.4741\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.9734 - mse: 14.9734\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.0752 - mse: 15.0752\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.2278 - mse: 9.2278\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.6554 - mse: 4.6554\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.7776 - mse: 3.7776\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.9260 - mse: 4.9260\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.1825 - mse: 18.1825\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.7737 - mse: 20.7737\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 13.2052 - mse: 13.2052\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.8798 - mse: 11.8798\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.7689 - mse: 11.7689\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.2308 - mse: 10.2308\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.1885 - mse: 14.1885\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.6970 - mse: 16.6970\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.5115 - mse: 11.5115\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.2632 - mse: 5.2632\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7511 - mse: 2.7511\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.5232 - mse: 3.5232\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.2774 - mse: 10.2774\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 25.3403 - mse: 25.3403\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 12.8839 - mse: 12.8839\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.8284 - mse: 8.8284\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.6634 - mse: 7.6634\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.5768 - mse: 4.5768\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.5425 - mse: 3.5425\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.5248 - mse: 4.5248\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 10.8402 - mse: 10.8402\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 33.0747 - mse: 33.0747\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 31.5718 - mse: 31.5718\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5226 - mse: 8.5226\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.7805 - mse: 3.7805\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.2713 - mse: 2.2713\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.9726 - mse: 1.9725\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6855 - mse: 1.6855\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 5.3240 - mse: 5.3240\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 26.2182 - mse: 26.2182\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.1332 - mse: 25.1332\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.0620 - mse: 13.0620\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 10.8039 - mse: 10.8039\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 6.2224 - mse: 6.2224\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.0823 - mse: 5.0823\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.3946 - mse: 8.3946\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 10.3787 - mse: 10.3787\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 17.3570 - mse: 17.3570\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 16.1007 - mse: 16.1007\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 11.0501 - mse: 11.0501\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 12.5290 - mse: 12.5290\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 13.9020 - mse: 13.9020\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 10.3711 - mse: 10.3711\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 6.4598 - mse: 6.4598\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2985 - mse: 4.2985\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.4229 - mse: 5.4229\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 14.8025 - mse: 14.8025\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 19.8003 - mse: 19.8003\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 23.8785 - mse: 23.8785\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 4.7776 - mse: 4.7776\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.1496 - mse: 2.1496\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1614 - mse: 2.1614\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 6.4095 - mse: 6.4095\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.3921 - mse: 21.3921\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 26.1630 - mse: 26.1630\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.4582 - mse: 8.4582\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 5.3663 - mse: 5.3663\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 6.6105 - mse: 6.6105\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 9.8157 - mse: 9.8157\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.5629 - mse: 13.5629\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 9.4765 - mse: 9.4765\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.9823 - mse: 8.9823\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.4850 - mse: 6.4850\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 12.7784 - mse: 12.7784\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 17.8516 - mse: 17.8516\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.6387 - mse: 13.6387\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 16.5752 - mse: 16.5752\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 12.8490 - mse: 12.8490\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 5.5052 - mse: 5.5052\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.9825 - mse: 8.9825\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 8.0600 - mse: 8.0600\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.4060 - mse: 11.4060\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 17.9107 - mse: 17.9107\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 20.0363 - mse: 20.0363\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 26.1758 - mse: 26.1758\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.1494 - mse: 9.1494\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 5.6729 - mse: 5.6729\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7040 - mse: 2.7040\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.7699 - mse: 1.7699\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 2.5017 - mse: 2.5017\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 10.1817 - mse: 10.1817\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 33.6454 - mse: 33.6454\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 19.8798 - mse: 19.8798\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 6.9777 - mse: 6.9777\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.3986 - mse: 3.3986\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.9487 - mse: 2.9487\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.7295 - mse: 1.7295\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.2128 - mse: 2.2128\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 16.7221 - mse: 16.7221\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 44.2916 - mse: 44.2916\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.5883 - mse: 7.5883\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.7384 - mse: 2.7384\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1275 - mse: 2.1275\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4864 - mse: 2.4864\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3250 - mse: 2.3250\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.0763 - mse: 3.0763\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 13.3860 - mse: 13.3860\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 34.9502 - mse: 34.9502\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.0452 - mse: 18.0452\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 5.9026 - mse: 5.9026\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7093 - mse: 2.7093\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7387 - mse: 2.7387\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 6.5541 - mse: 6.5541\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 26.9816 - mse: 26.9816\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.6001 - mse: 25.6001\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 12.9950 - mse: 12.9950\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 4.3030 - mse: 4.3030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1687c8e50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.78321904, 1.2191149 , 0.7829328 ],\n",
       "        [0.7357272 , 1.2666777 , 0.73542976],\n",
       "        [0.7587743 , 1.2436014 , 0.7584809 ],\n",
       "        [0.7399096 , 1.262459  , 0.73961765],\n",
       "        [0.87216467, 1.1297032 , 0.87194175],\n",
       "        [0.86290854, 1.1390507 , 0.86267155]], dtype=float32),\n",
       " array([-0.17468476,  0.17676489, -0.17493707], dtype=float32),\n",
       " array([[-0.7475918 ],\n",
       "        [ 1.3148766 ],\n",
       "        [-0.95912707]], dtype=float32),\n",
       " array([0.17591994], dtype=float32)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x16880f3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 15:28:13.822397: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[18.471478]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>18.471478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.161646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>17.160315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>20.979366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.787192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit\n",
       "abbrev                             \n",
       "AL       18.8             18.471478\n",
       "AK       18.1             17.161646\n",
       "AZ       18.6             17.160315\n",
       "AR       22.4             20.979366\n",
       "CA       12.0             12.787192"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel = df[['total']].copy()\n",
    "dfsel['pred_zeros_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.016406947568492"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_zeros_after_fit)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to `glorot_uniform` (default)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with the Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=558\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=558\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `sigmoid` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81e8d14310>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f81e00f65e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.909626</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.232151</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.393391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>19.506733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>14.266361</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_sigmoid\n",
       "abbrev                                           \n",
       "AL       18.8             17.909626           0.0\n",
       "AK       18.1             17.232151           0.0\n",
       "AZ       18.6             16.393391           0.0\n",
       "AR       22.4             19.506733           0.0\n",
       "CA       12.0             14.266361           0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_sigmoid'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265.98803921568634"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.7248485 ,  0.36856735,  0.63146174],\n",
       "        [ 0.07509726,  0.33351755, -0.30260724],\n",
       "        [-0.7252705 , -0.55833864,  0.15868711],\n",
       "        [-0.39576796, -0.33873704, -0.768407  ],\n",
       "        [ 0.64309967, -0.5359219 , -0.67604953],\n",
       "        [ 0.11188209, -0.31420344,  0.22522306]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.3183049 ],\n",
       "        [ 1.1042565 ],\n",
       "        [-0.02906787]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `linear` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81e8d14310>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f81e00f65e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.909626</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.232151</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.393391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>19.506733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>14.266361</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_sigmoid\n",
       "abbrev                                           \n",
       "AL       18.8             17.909626           0.0\n",
       "AK       18.1             17.232151           0.0\n",
       "AZ       18.6             16.393391           0.0\n",
       "AR       22.4             19.506733           0.0\n",
       "CA       12.0             14.266361           0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_sigmoid'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265.98803921568634"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.7248485 ,  0.36856735,  0.63146174],\n",
       "        [ 0.07509726,  0.33351755, -0.30260724],\n",
       "        [-0.7252705 , -0.55833864,  0.15868711],\n",
       "        [-0.39576796, -0.33873704, -0.768407  ],\n",
       "        [ 0.64309967, -0.5359219 , -0.67604953],\n",
       "        [ 0.11188209, -0.31420344,  0.22522306]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.3183049 ],\n",
       "        [ 1.1042565 ],\n",
       "        [-0.02906787]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `tanh` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `relu` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are the predictions changing? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/optimizers/#available-optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers comparison in GIF ‚Üí https://mlfromscratch.com/optimizers-explained/#adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tesla's Neural Network Models is composed of 48 models trainned in 70.000 hours of GPU ‚Üí https://tesla.com/ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Year with a 8 GPU Computer ‚Üí https://twitter.com/thirdrowtesla/status/1252723358342377472"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Gradient Descent `SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `compile()` the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f81e8d0cf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_gsd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>17.909626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>17.232151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.393391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>19.506733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>14.266361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit  pred_sigmoid  pred_gsd\n",
       "abbrev                                                     \n",
       "AL       18.8             17.909626           0.0       0.0\n",
       "AK       18.1             17.232151           0.0       0.0\n",
       "AZ       18.6             16.393391           0.0       0.0\n",
       "AR       22.4             19.506733           0.0       0.0\n",
       "CA       12.0             14.266361           0.0       0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_gsd'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'pred_sgd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p1/lnbb0qwd743gqfmgynnt5w3m0000gn/T/ipykernel_36994/4078616456.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfsel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdfsel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_sgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'pred_sgd'"
     ]
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sgd)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### View History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `ADAM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `RMSPROP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it take different times to get the best accuracy? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `binary_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sparse_categorical_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mean_absolute_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mean_squared_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the end, what should be a feasible configuration of the Neural Network for this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `kernel_initializer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `activation` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `optimizer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `loss` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network's importance to find **Non-Linear Patterns** in the Data\n",
    "\n",
    "> - The number of Neurons & Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.87287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Mathematical Formula\n",
    "- Weights / Kernel Initializer\n",
    "- Loss Function\n",
    "- Activation Function\n",
    "- Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What cannot you change arbitrarily of a Neural Network?\n",
    "\n",
    "- Input Neurons\n",
    "- Output Neurons\n",
    "- Loss Functions\n",
    "- Activation Functions"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Jes√∫s L√≥pez @sotastica"
   }
  ],
  "interpreter": {
   "hash": "a2b8701b642343483c5f5e717bcb0768c6b951acf38d76a6fc8ea01492bda71d"
  },
  "kernelspec": {
   "display_name": "DeepLearning Python",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
